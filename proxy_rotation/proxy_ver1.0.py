from bs4 import BeautifulSoup as bs
from pathlib import Path
from openpyxl import Workbook
from fake_useragent import UserAgent
from requests.exceptions import RequestException, Timeout, ConnectTimeout, ReadTimeout
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.webdriver.common.proxy import Proxy, ProxyType
import time
import os
import re
import requests as rq
import math
import sys
import random
import itertools
import json
from urllib.parse import urlencode


class ProxyRotator:
    def __init__(self, proxy_list=None):
        """
        í”„ë¡ì‹œ ë¡œí…Œì´í„° ì´ˆê¸°í™”
        proxy_list: ['ip:port:username:password', ...] í˜•íƒœì˜ í”„ë¡ì‹œ ë¦¬ìŠ¤íŠ¸
        """
        self.proxy_list = proxy_list if proxy_list else []
        self.proxy_cycle = itertools.cycle(self.proxy_list) if self.proxy_list else None
        self.current_proxy = None
        self.failed_proxies = set()

    def get_next_proxy(self):
        """ë‹¤ìŒ í”„ë¡ì‹œë¥¼ ë°˜í™˜"""
        if not self.proxy_cycle:
            return None

        # ì‹¤íŒ¨í•œ í”„ë¡ì‹œê°€ ì•„ë‹Œ ê²ƒì„ ì°¾ì„ ë•Œê¹Œì§€ ìˆœí™˜
        for _ in range(len(self.proxy_list)):
            proxy = next(self.proxy_cycle)
            if proxy not in self.failed_proxies:
                self.current_proxy = proxy
                # í”„ë¡ì‹œ ì •ë³´ ì¶œë ¥ (IPë§Œ í‘œì‹œ)
                proxy_ip = proxy.split(':')[0]
                print(f"[PROXY] í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ í”„ë¡ì‹œ: {proxy_ip}")
                return proxy

        # ëª¨ë“  í”„ë¡ì‹œê°€ ì‹¤íŒ¨í–ˆë‹¤ë©´ ì‹¤íŒ¨ ëª©ë¡ì„ ì´ˆê¸°í™”í•˜ê³  ë‹¤ì‹œ ì‹œë„
        print("[WARNING] ëª¨ë“  í”„ë¡ì‹œê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì‹¤íŒ¨ ëª©ë¡ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
        self.failed_proxies.clear()
        self.current_proxy = next(self.proxy_cycle)
        proxy_ip = self.current_proxy.split(':')[0]
        print(f"[PROXY] í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ í”„ë¡ì‹œ: {proxy_ip}")
        return self.current_proxy

    def mark_proxy_failed(self, proxy):
        """í”„ë¡ì‹œë¥¼ ì‹¤íŒ¨ ëª©ë¡ì— ì¶”ê°€"""
        self.failed_proxies.add(proxy)
        proxy_ip = proxy.split(':')[0]
        print(f"[WARNING] í”„ë¡ì‹œ ì‹¤íŒ¨ë¡œ í‘œì‹œ: {proxy_ip} ({len(self.failed_proxies)}/{len(self.proxy_list)} ì‹¤íŒ¨)")

    def get_proxy_dict(self, proxy_string):
        """í”„ë¡ì‹œ ë¬¸ìì—´ì„ requestsìš© ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        if not proxy_string:
            return None

        parts = proxy_string.split(':')
        if len(parts) == 2:  # ip:port
            ip, port = parts
            return {
                'http': f'http://{ip}:{port}',
                'https': f'http://{ip}:{port}'
            }
        elif len(parts) == 4:  # ip:port:username:password
            ip, port, username, password = parts
            return {
                'http': f'http://{username}:{password}@{ip}:{port}',
                'https': f'http://{username}:{password}@{ip}:{port}'
            }
        return None


class ChromeDriver:
    def __init__(self, proxy_rotator=None) -> None:
        self.proxy_rotator = proxy_rotator
        self.ua = UserAgent()
        self.set_options()
        self.set_driver()

    def set_options(self) -> None:
        self.options = Options()
        self.options.add_argument("--headless")
        self.options.add_argument("lang=ko_KR")

        # fake_useragent ì‚¬ìš©
        user_agent = self.ua.random
        self.options.add_argument(f"user-agent={user_agent}")
        print(f"[DEBUG] ì‚¬ìš© ì¤‘ì¸ User-Agent: {user_agent}")

        # ë” ë§ì€ ë¸Œë¼ìš°ì € ì˜µì…˜ ì¶”ê°€ë¡œ íƒì§€ ë°©ì§€
        self.options.add_argument("--log-level=3")
        self.options.add_argument("--no-sandbox")
        self.options.add_argument("--disable-dev-shm-usage")
        self.options.add_argument("--disable-blink-features=AutomationControlled")
        self.options.add_argument("--exclude-switches=enable-automation")
        self.options.add_argument("--disable-extensions")
        self.options.add_argument("--no-first-run")
        self.options.add_argument("--disable-default-apps")
        self.options.add_argument("--disable-infobars")
        self.options.add_experimental_option("detach", True)
        self.options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.options.add_experimental_option('useAutomationExtension', False)

        # í”„ë¡ì‹œ ì„¤ì •
        if self.proxy_rotator:
            proxy = self.proxy_rotator.get_next_proxy()
            if proxy:
                parts = proxy.split(':')
                if len(parts) >= 2:
                    ip, port = parts[0], parts[1]
                    self.options.add_argument(f'--proxy-server=http://{ip}:{port}')
                    print(f"[DEBUG] Selenium í”„ë¡ì‹œ ì„¤ì •: {ip}:{port}")

    def set_driver(self) -> None:
        self.driver = webdriver.Chrome(options=self.options)
        # WebDriver íƒì§€ ë°©ì§€
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    def refresh_with_new_proxy(self):
        """ìƒˆë¡œìš´ í”„ë¡ì‹œë¡œ ë“œë¼ì´ë²„ ì¬ì‹œì‘"""
        if hasattr(self, 'driver') and self.driver:
            self.driver.quit()
        self.set_options()
        self.set_driver()


class Coupang:
    @staticmethod
    def get_product_code(url: str) -> str:
        prod_code: str = url.split("products/")[-1].split("?")[0]
        return prod_code

    @staticmethod
    def get_soup_object(resp: rq.Response) -> bs:
        return bs(resp.text, "html.parser")

    def __del__(self) -> None:
        if hasattr(self, 'ch') and self.ch.driver:
            self.ch.driver.quit()

    def __init__(self, proxy_list=None) -> None:
        self.base_review_url: str = "https://www.coupang.com/vp/product/reviews"
        self.sd = SaveData()
        self.retries = 8  # ì¬ì‹œë„ íšŸìˆ˜ ì¤„ì„
        self.delay_min = 2.0  # ìµœì†Œ ë”œë ˆì´ ì¦ê°€
        self.delay_max = 8.0  # ìµœëŒ€ ë”œë ˆì´ ì¦ê°€
        self.page_delay_min = 3.0  # í˜ì´ì§€ ê°„ ìµœì†Œ ë”œë ˆì´ ì¦ê°€
        self.page_delay_max = 10.0  # í˜ì´ì§€ ê°„ ìµœëŒ€ ë”œë ˆì´ ì¦ê°€

        # íƒ€ì„ì•„ì›ƒ ê´€ë ¨ ì„¤ì •
        self.consecutive_timeouts = 0
        self.max_consecutive_timeouts = 3  # ì—°ì† íƒ€ì„ì•„ì›ƒ í—ˆìš© íšŸìˆ˜ ê°ì†Œ
        self.long_wait_min = 300  # ê¸´ ëŒ€ê¸° ì‹œê°„ ì¤„ì„ (5ë¶„)
        self.long_wait_max = 420  # ê¸´ ëŒ€ê¸° ì‹œê°„ ì¤„ì„ (7ë¶„)

        # í”„ë¡ì‹œ ë¡œí…Œì´í„° ì´ˆê¸°í™”
        self.proxy_rotator = ProxyRotator(proxy_list)

        # fake_useragent ì´ˆê¸°í™”
        self.ua = UserAgent()

        # ë” ì •êµí•œ í—¤ë” ì„¤ì •
        self.base_headers = {
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "cache-control": "max-age=0",
            "sec-ch-ua": '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "document",
            "sec-fetch-mode": "navigate",
            "sec-fetch-site": "same-origin",
            "sec-fetch-user": "?1",
            "upgrade-insecure-requests": "1",
            "dnt": "1",
        }

        # ì¿ í‚¤ ì €ì¥ìš© ì„¸ì…˜
        self.session = rq.Session()

        # í—¤ë”ì— ëœë¤ User-Agent ì ìš©
        self.update_headers()

        self.ch = ChromeDriver(self.proxy_rotator)
        self.page_title = None

    def get_realistic_headers(self):
        """ì‹¤ì œ ë¸Œë¼ìš°ì €ì™€ ìœ ì‚¬í•œ í—¤ë” ìƒì„±"""
        headers = self.base_headers.copy()
        headers["user-agent"] = self.ua.random

        # ëœë¤ ìš”ì†Œ ì¶”ê°€
        if random.choice([True, False]):
            headers["x-requested-with"] = "XMLHttpRequest"

        # ì¿ íŒ¡ íŠ¹í™” í—¤ë”
        headers.update({
            "x-coupang-target-market": "KR",
            "x-coupang-accept-language": "ko-KR",
        })

        return headers

    def update_headers(self):
        """í—¤ë”ë¥¼ ìƒˆë¡œìš´ User-Agentë¡œ ì—…ë°ì´íŠ¸"""
        self.headers = self.get_realistic_headers()
        print(f"[DEBUG] í—¤ë” User-Agent ì—…ë°ì´íŠ¸: {self.headers['user-agent'][:50]}...")

    def get_session_with_proxy(self):
        """í”„ë¡ì‹œê°€ ì ìš©ëœ requests ì„¸ì…˜ ë°˜í™˜"""
        session = rq.Session()
        session.headers.update(self.headers)

        # ë” í˜„ì‹¤ì ì¸ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        session.timeout = (10, 30)  # ì—°ê²° íƒ€ì„ì•„ì›ƒ 10ì´ˆ, ì½ê¸° íƒ€ì„ì•„ì›ƒ 30ì´ˆ

        if self.proxy_rotator and self.proxy_rotator.proxy_list:
            proxy = self.proxy_rotator.get_next_proxy()
            if proxy:
                proxy_dict = self.proxy_rotator.get_proxy_dict(proxy)
                if proxy_dict:
                    session.proxies.update(proxy_dict)
                    print(f"[DEBUG] ìš”ì²­ì— í”„ë¡ì‹œ ì ìš©: {proxy}")

        return session

    def warm_up_session(self, prod_code):
        """ì„¸ì…˜ì„ ì˜ˆì—´í•˜ì—¬ ì¿ íŒ¡ ì‚¬ì´íŠ¸ì™€ì˜ ì—°ê²°ì„ ì„¤ì •"""
        try:
            print("[INFO] ì„¸ì…˜ ì˜ˆì—´ ì¤‘...")

            # ë©”ì¸ í˜ì´ì§€ ë¨¼ì € ë°©ë¬¸
            main_url = "https://www.coupang.com"
            session = self.get_session_with_proxy()

            # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸
            resp = session.get(main_url, timeout=15)
            if resp.status_code == 200:
                print("[DEBUG] ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ ì„±ê³µ")

                # ì¿ í‚¤ ì—…ë°ì´íŠ¸
                self.session.cookies.update(resp.cookies)

                # ì ì‹œ ëŒ€ê¸°
                time.sleep(random.uniform(2, 4))

                # ìƒí’ˆ í˜ì´ì§€ ë°©ë¬¸
                product_url = f"https://www.coupang.com/vp/products/{prod_code}"
                resp2 = session.get(product_url, timeout=15)

                if resp2.status_code == 200:
                    print("[DEBUG] ìƒí’ˆ í˜ì´ì§€ ë°©ë¬¸ ì„±ê³µ")
                    self.session.cookies.update(resp2.cookies)
                    return True

        except Exception as e:
            print(f"[WARNING] ì„¸ì…˜ ì˜ˆì—´ ì‹¤íŒ¨: {e}")

        return False

    def get_product_title(self, prod_code: str) -> str:
        """ìƒí’ˆëª…ë§Œ ê°„ë‹¨í•˜ê²Œ ì¶”ì¶œ"""
        url = f"https://www.coupang.com/vp/products/{prod_code}"
        print(f"[DEBUG] ìƒí’ˆ í˜ì´ì§€ ì ‘ì† ì¤‘: {url}")

        try:
            # ì—¬ëŸ¬ ë²ˆ ì‹œë„
            for attempt in range(3):
                try:
                    self.ch.driver.get(url=url)

                    print("[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
                    WebDriverWait(self.ch.driver, 30).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )

                    loading_delay = random.uniform(3.0, 6.0)  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                    print(f"[DEBUG] {loading_delay:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(loading_delay)

                    page_source: str = self.ch.driver.page_source
                    soup = bs(page_source, "html.parser")

                    # ë” ë§ì€ ìƒí’ˆëª… ì„ íƒì ì‹œë„
                    title_selectors = [
                        "h1.prod-buy-header__title",
                        ".prod-buy-header__title",
                        "h1[class*='title']",
                        ".product-title",
                        "h1",
                        ".prod-title",
                        "[data-testid='product-title']",
                        ".product-name"
                    ]

                    for selector in title_selectors:
                        title_elem = soup.select_one(selector)
                        if title_elem and title_elem.text.strip():
                            title = title_elem.text.strip()
                            print(f"[DEBUG] ìƒí’ˆëª… ë°œê²¬: {title}")
                            return title

                    print(f"[WARNING] ì‹œë„ {attempt + 1}/3 - ìƒí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    if attempt < 2:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´
                        time.sleep(random.uniform(3, 5))

                except Exception as e:
                    print(f"[ERROR] ì‹œë„ {attempt + 1}/3 ì‹¤íŒ¨: {e}")
                    if attempt < 2:
                        time.sleep(random.uniform(3, 5))

            print("[WARNING] ëª¨ë“  ì‹œë„ í›„ì—ë„ ìƒí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "ìƒí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨"

        except Exception as e:
            print(f"[ERROR] get_product_title ì—ëŸ¬: {e}")
            return "ìƒí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨"

    def is_timeout_error(self, exception) -> bool:
        """íƒ€ì„ì•„ì›ƒ ê´€ë ¨ ì˜ˆì™¸ì¸ì§€ í™•ì¸"""
        return isinstance(exception, (Timeout, ConnectTimeout, ReadTimeout)) or \
            (isinstance(exception, RequestException) and "timeout" in str(exception).lower())

    def handle_consecutive_timeouts(self) -> None:
        """ì—°ì† íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬"""
        if self.consecutive_timeouts >= self.max_consecutive_timeouts:
            wait_time = random.uniform(self.long_wait_min, self.long_wait_max)
            wait_minutes = wait_time / 60
            print(f"[WARNING] ì—°ì† {self.consecutive_timeouts}íšŒ íƒ€ì„ì•„ì›ƒ ë°œìƒ!")
            print(f"[INFO] ì„œë²„ ì•ˆì •í™”ë¥¼ ìœ„í•´ {wait_minutes:.1f}ë¶„ ëŒ€ê¸°í•©ë‹ˆë‹¤...")

            remaining_time = wait_time
            while remaining_time > 0:
                minutes_left = remaining_time / 60
                print(f"[INFO] ë‚¨ì€ ëŒ€ê¸° ì‹œê°„: {minutes_left:.1f}ë¶„")

                sleep_duration = min(30, remaining_time)
                time.sleep(sleep_duration)
                remaining_time -= sleep_duration

            print(f"[INFO] ëŒ€ê¸° ì™„ë£Œ! í¬ë¡¤ë§ì„ ì¬ê°œí•©ë‹ˆë‹¤.")
            self.consecutive_timeouts = 0

    def start(self) -> None:
        self.sd.create_directory()
        URL: str = self.input_review_url()

        if '#' in URL:
            URL = URL.split('#')[0]
            print(f"[DEBUG] URL fragment ì œê±°: {URL}")

        prod_code: str = self.get_product_code(url=URL)
        print(f"[DEBUG] ìƒí’ˆ ì½”ë“œ: {prod_code}")

        # ì„¸ì…˜ ì˜ˆì—´
        self.warm_up_session(prod_code)

        try:
            self.title = self.get_product_title(prod_code=prod_code)
            print(f"[INFO] ìƒí’ˆëª…: {self.title}")
        except Exception as e:
            print(f"[ERROR] ìƒí’ˆëª…ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            self.title = "ìƒí’ˆëª… ë¯¸í™•ì¸"

        print(f"[INFO] ëª¨ë“  ë¦¬ë·° í˜ì´ì§€ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")

        success_count = 0
        current_page = 1
        consecutive_empty_pages = 0
        max_empty_pages = 3

        while consecutive_empty_pages < max_empty_pages:
            payload = {
                "productId": prod_code,
                "page": current_page,
                "size": 5,
                "sortBy": "ORDER_SCORE_ASC",
                "ratings": "",
                "q": "",
                "viRoleCode": 2,
                "ratingSummary": True,
            }

            result = self.fetch(payload=payload)

            if result:
                success_count += 1
                consecutive_empty_pages = 0
            else:
                consecutive_empty_pages += 1
                print(f"[WARNING] í˜ì´ì§€ {current_page}ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({consecutive_empty_pages}/{max_empty_pages})")

            current_page += 1

            if result and consecutive_empty_pages == 0:
                short_delay = random.uniform(1.0, 3.0)  # ì§§ì€ ë”œë ˆì´ë„ ì¦ê°€
                time.sleep(short_delay)

            if current_page > 500:  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ê°ì†Œ
                print("[INFO] ìµœëŒ€ í˜ì´ì§€ ìˆ˜(500)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                break

        print(f"[INFO] ì´ {success_count}ê°œ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ (ì´ {current_page - 1}í˜ì´ì§€ ì‹œë„)")

    def fetch(self, payload: dict) -> bool:
        now_page: int = payload["page"]
        print(f"\n[INFO] Start crawling page {now_page} ...")
        attempt: int = 0

        while attempt < self.retries:
            try:
                # ë§¤ ìš”ì²­ë§ˆë‹¤ ìƒˆë¡œìš´ User-Agent ì‚¬ìš©
                if attempt > 0:  # ì¬ì‹œë„ ì‹œì—ë§Œ User-Agent ë³€ê²½
                    self.update_headers()

                session = self.get_session_with_proxy()

                # ì„¸ì…˜ì— ê¸°ì¡´ ì¿ í‚¤ ì ìš©
                session.cookies.update(self.session.cookies)

                # Referer í—¤ë” ì¶”ê°€
                session.headers.update({
                    "Referer": f"https://www.coupang.com/vp/products/{payload['productId']}"
                })

                resp = session.get(
                    url=self.base_review_url,
                    params=payload,
                    timeout=(15, 30),  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
                )

                self.consecutive_timeouts = 0

                if resp.status_code == 403:
                    print(f"[ERROR] HTTP 403 ì‘ë‹µ - í”„ë¡ì‹œê°€ ì°¨ë‹¨ë¨")
                    if self.proxy_rotator and self.proxy_rotator.current_proxy:
                        self.proxy_rotator.mark_proxy_failed(self.proxy_rotator.current_proxy)
                    attempt += 1
                    continue
                elif resp.status_code != 200:
                    print(f"[ERROR] HTTP {resp.status_code} ì‘ë‹µ")
                    attempt += 1
                    continue

                html = resp.text
                soup = bs(html, "html.parser")

                # ë””ë²„ê¹…: ì‘ë‹µ ë‚´ìš© í™•ì¸
                if now_page == 1 and len(html) < 10000:
                    print(f"[DEBUG] ì‘ë‹µ HTML ê¸¸ì´: {len(html)}")
                    print(f"[DEBUG] ì‘ë‹µ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {html[:500]}")

                if self.page_title is None:
                    first_review = soup.select_one("article.sdp-review__article__list")
                    if first_review:
                        title_elem = first_review.select_one("div.sdp-review__article__list__info__product-info__name")
                        self.page_title = title_elem.text.strip() if title_elem else self.title
                    else:
                        self.page_title = self.title

                articles = soup.select("article.sdp-review__article__list")
                article_length = len(articles)

                if article_length == 0:
                    print(f"[WARNING] í˜ì´ì§€ {now_page}ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    if now_page == 1:
                        print("[DEBUG] ì²« í˜ì´ì§€ HTML êµ¬ì¡° í™•ì¸:")
                        print(f"  - ì „ì²´ ê¸¸ì´: {len(html)} ë¬¸ì")
                        print(f"  - 'review' í¬í•¨ íšŸìˆ˜: {html.lower().count('review')}")
                        print(f"  - 'article' í¬í•¨ íšŸìˆ˜: {html.lower().count('article')}")

                        # ì°¨ë‹¨ ì—¬ë¶€ í™•ì¸
                        blocked_indicators = [
                            "access denied", "blocked", "forbidden",
                            "captcha", "robot", "bot", "security"
                        ]

                        html_lower = html.lower()
                        for indicator in blocked_indicators:
                            if indicator in html_lower:
                                print(f"[WARNING] ì°¨ë‹¨ ê°ì§€: '{indicator}' ë°œê²¬")
                                break

                        alt_selectors = [
                            "article[class*='review']",
                            "div[class*='review-item']",
                            "div[class*='review-list']",
                            ".review-item",
                            "[data-review-id]"
                        ]

                        for selector in alt_selectors:
                            elements = soup.select(selector)
                            if elements:
                                print(f"[DEBUG] ëŒ€ì•ˆ ì„ íƒì '{selector}' ë°œê²¬: {len(elements)}ê°œ")
                    return False

                print(f"[SUCCESS] í˜ì´ì§€ {now_page}ì—ì„œ {article_length}ê°œ ë¦¬ë·° ë°œê²¬")

                # ë¦¬ë·° ë°ì´í„° ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
                for idx in range(article_length):
                    dict_data: dict[str, str | int] = dict()

                    review_date_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__reg-date"
                    )
                    review_date = review_date_elem.text.strip() if review_date_elem else "-"

                    user_name_elem = articles[idx].select_one(
                        "span.sdp-review__article__list__info__user__name"
                    )
                    user_name = user_name_elem.text.strip() if user_name_elem else "-"

                    rating_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__star-orange"
                    )
                    if rating_elem and rating_elem.get("data-rating"):
                        try:
                            rating = int(rating_elem.get("data-rating"))
                        except (ValueError, TypeError):
                            rating = 0
                    else:
                        rating = 0

                    prod_name_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__name"
                    )
                    prod_name = prod_name_elem.text.strip() if prod_name_elem else "-"

                    headline_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__headline"
                    )
                    headline = headline_elem.text.strip() if headline_elem else "ë“±ë¡ëœ í—¤ë“œë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤"

                    review_content_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__review__content.js_reviewArticleContent"
                    )
                    if review_content_elem:
                        review_content = re.sub("[\n\t]", "", review_content_elem.text.strip())
                    else:
                        review_content_elem = articles[idx].select_one(
                            "div.sdp-review__article__list__review > div"
                        )
                        if review_content_elem:
                            review_content = re.sub("[\n\t]", "", review_content_elem.text.strip())
                        else:
                            review_content = "ë“±ë¡ëœ ë¦¬ë·°ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"

                    answer_elem = articles[idx].select_one(
                        "span.sdp-review__article__list__survey__row__answer"
                    )
                    answer = answer_elem.text.strip() if answer_elem else "ë§› í‰ê°€ ì—†ìŒ"

                    helpful_count_elem = articles[idx].select_one("span.js_reviewArticleHelpfulCount")
                    helpful_count = helpful_count_elem.text.strip() if helpful_count_elem else "0"

                    seller_name_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__seller_name"
                    )
                    if seller_name_elem:
                        seller_name = seller_name_elem.text.replace("íŒë§¤ì: ", "").strip()
                    else:
                        seller_name = "-"

                    review_images = articles[idx].select("div.sdp-review__article__list__attachment__list img")
                    image_count = len(review_images)

                    dict_data["title"] = self.page_title
                    dict_data["prod_name"] = prod_name
                    dict_data["review_date"] = review_date
                    dict_data["user_name"] = user_name
                    dict_data["rating"] = rating
                    dict_data["headline"] = headline
                    dict_data["review_content"] = review_content
                    dict_data["answer"] = answer
                    dict_data["helpful_count"] = helpful_count
                    dict_data["seller_name"] = seller_name
                    dict_data["image_count"] = image_count

                    self.sd.save(datas=dict_data)
                    print(f"[SUCCESS] ë¦¬ë·° ì €ì¥ ì™„ë£Œ: {user_name} - {rating}ì ")

                page_delay = random.uniform(self.page_delay_min, self.page_delay_max)
                print(f"[DEBUG] ë‹¤ìŒ í˜ì´ì§€ê¹Œì§€ {page_delay:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(page_delay)
                return True

            except RequestException as e:
                attempt += 1

                # í”„ë¡ì‹œ ê´€ë ¨ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
                if "403" in str(e) or "proxy" in str(e).lower() or "connection" in str(e).lower():
                    if self.proxy_rotator and self.proxy_rotator.current_proxy:
                        self.proxy_rotator.mark_proxy_failed(self.proxy_rotator.current_proxy)
                        print("[INFO] ë‹¤ë¥¸ í”„ë¡ì‹œë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")

                if self.is_timeout_error(e):
                    self.consecutive_timeouts += 1
                    print(f"[ERROR] íƒ€ì„ì•„ì›ƒ ë°œìƒ (ì—°ì† {self.consecutive_timeouts}íšŒ): {e}")

                    if self.consecutive_timeouts >= self.max_consecutive_timeouts:
                        self.handle_consecutive_timeouts()
                else:
                    self.consecutive_timeouts = 0
                    print(f"[ERROR] ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")

                print(f"[ERROR] Attempt {attempt}/{self.retries} failed")
                if attempt < self.retries:
                    retry_delay = random.uniform(self.delay_min, self.delay_max)
                    print(f"[DEBUG] {retry_delay:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(retry_delay)
                else:
                    print(f"[ERROR] ìµœëŒ€ ìš”ì²­ íšŸìˆ˜ ì´ˆê³¼! í˜ì´ì§€ {now_page} í¬ë¡¤ë§ ì‹¤íŒ¨.")
                    return False
            except Exception as e:
                print(f"[ERROR] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                self.consecutive_timeouts = 0
                return False

        return False

    @staticmethod
    def clear_console() -> None:
        command: str = "clear"
        if os.name in ("nt", "dos"):
            command = "cls"
        try:
            os.system(command=command)
        except:
            pass

    def input_review_url(self) -> str:
        while True:
            try:
                self.clear_console()
            except:
                pass

            review_url: str = input(
                "ì›í•˜ì‹œëŠ” ìƒí’ˆì˜ URL ì£¼ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”\n\n"
                "Ex)\n"
                "https://www.coupang.com/vp/products/7335597976?itemId=18741704367&vendorItemId=85873964906\n\n"
                "URL: "
            )
            if not review_url.strip():
                print("[ERROR] URL ì£¼ì†Œê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                time.sleep(2)
                continue

            if "coupang.com" not in review_url:
                print("[ERROR] ì˜¬ë°”ë¥¸ ì¿ íŒ¡ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”")
                time.sleep(2)
                continue

            return review_url.strip()


class SaveData:
    def __init__(self) -> None:
        self.wb: Workbook = Workbook()
        self.ws = self.wb.active
        self.ws.append([
            "ìƒí’ˆëª…", "êµ¬ë§¤ìƒí’ˆëª…", "ì‘ì„±ì¼ì", "êµ¬ë§¤ìëª…", "í‰ì ",
            "í—¤ë“œë¼ì¸", "ë¦¬ë·°ë‚´ìš©", "ë§›ë§Œì¡±ë„", "ë„ì›€ìˆ˜", "íŒë§¤ì", "ì´ë¯¸ì§€ìˆ˜"
        ])
        self.row: int = 2
        self.dir_name: str = "../Coupang-reviews"
        self.create_directory()

    def create_directory(self) -> None:
        if not os.path.exists(self.dir_name):
            os.makedirs(self.dir_name)
            print(f"[INFO] ë””ë ‰í† ë¦¬ ìƒì„±: {self.dir_name}")

    def save(self, datas: dict[str, str | int]) -> None:
        try:
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', datas["title"])
            file_name: str = os.path.join(self.dir_name, safe_title + ".xlsx")

            self.ws[f"A{self.row}"] = datas["title"]
            self.ws[f"B{self.row}"] = datas["prod_name"]
            self.ws[f"C{self.row}"] = datas["review_date"]
            self.ws[f"D{self.row}"] = datas["user_name"]
            self.ws[f"E{self.row}"] = datas["rating"]
            self.ws[f"F{self.row}"] = datas["headline"]
            self.ws[f"G{self.row}"] = datas["review_content"]
            self.ws[f"H{self.row}"] = datas["answer"]
            self.ws[f"I{self.row}"] = datas["helpful_count"]
            self.ws[f"J{self.row}"] = datas["seller_name"]
            self.ws[f"K{self.row}"] = datas["image_count"]

            self.row += 1
            self.wb.save(filename=file_name)

        except Exception as e:
            print(f"[ERROR] ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def __del__(self) -> None:
        try:
            if hasattr(self, 'wb'):
                self.wb.close()
        except:
            pass


def test_proxy(proxy_string):
    """í”„ë¡ì‹œ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        parts = proxy_string.split(':')
        if len(parts) == 4:
            ip, port, username, password = parts
            proxy_dict = {
                'http': f'http://{username}:{password}@{ip}:{port}',
                'https': f'http://{username}:{password}@{ip}:{port}'
            }
        else:
            return False

        response = rq.get('http://httpbin.org/ip', proxies=proxy_dict, timeout=10)
        if response.status_code == 200:
            return True
    except:
        pass
    return False


def get_proxy_list():
    """í”„ë¡ì‹œ ëª©ë¡ ë°˜í™˜"""
    # ì œê³µë°›ì€ í”„ë¡ì‹œ ëª©ë¡
    proxy_list = [
        "198.23.239.134:6540:daxvymvx:kn518nmfd34a",
        "207.244.217.165:6712:daxvymvx:kn518nmfd34a",
        "107.172.163.27:6543:daxvymvx:kn518nmfd34a",
        "161.123.152.115:6360:daxvymvx:kn518nmfd34a",
        "23.94.138.75:6349:daxvymvx:kn518nmfd34a",
        "216.10.27.159:6837:daxvymvx:kn518nmfd34a",
        "136.0.207.84:6661:daxvymvx:kn518nmfd34a",
        "64.64.118.149:6732:daxvymvx:kn518nmfd34a",
        "142.147.128.93:6593:daxvymvx:kn518nmfd34a",
        "154.36.110.199:6853:daxvymvx:kn518nmfd34a"
    ]

    print(f"[INFO] {len(proxy_list)}ê°œì˜ í”„ë¡ì‹œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("[NOTICE] HTTP 403 ì˜¤ë¥˜ê°€ ë°œìƒí•  ê²½ìš°:")
    print("  1. í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰ (n ì„ íƒ)")
    print("  2. VPN ì‚¬ìš©")
    print("  3. ë‹¤ë¥¸ residential í”„ë¡ì‹œ ì„œë¹„ìŠ¤ ì´ìš©")
    print()

    # í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_proxy = input("í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").lower().strip()

    if use_proxy == 'n':
        print("[INFO] í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return None
    else:
        print("[INFO] í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.")

        # í”„ë¡ì‹œ í…ŒìŠ¤íŠ¸ ì—¬ë¶€ í™•ì¸
        test_proxies = input("í”„ë¡ì‹œ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower().strip()

        if test_proxies == 'y':
            print("\n[INFO] í”„ë¡ì‹œ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
            working_proxies = []

            for i, proxy in enumerate(proxy_list, 1):
                print(f"[TEST] {i}/{len(proxy_list)} - {proxy.split(':')[0]}:{proxy.split(':')[1]} í…ŒìŠ¤íŠ¸ ì¤‘...", end='')
                if test_proxy(proxy):
                    print(" âœ… ì„±ê³µ")
                    working_proxies.append(proxy)
                else:
                    print(" âŒ ì‹¤íŒ¨")

            if working_proxies:
                print(f"\n[SUCCESS] {len(working_proxies)}/{len(proxy_list)}ê°œ í”„ë¡ì‹œê°€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
                return working_proxies
            else:
                print("\n[ERROR] ì‘ë™í•˜ëŠ” í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                return None
        else:
            return proxy_list


if __name__ == "__main__":
    try:
        print("=" * 70)
        print("ğŸ›’ ì¿ íŒ¡ ë¦¬ë·° í¬ë¡¤ëŸ¬ v3.3 (ê°•í™”ëœ ìš°íšŒ ê¸°ëŠ¥)")
        print("=" * 70)

        # í”„ë¡ì‹œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        proxy_list = get_proxy_list()

        coupang = Coupang(proxy_list=proxy_list)
        coupang.start()

        print("\n" + "=" * 70)
        print("âœ… í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ“ ê²°ê³¼ íŒŒì¼ì€ 'Coupang-reviews' í´ë”ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
        print("=" * 70)

    except KeyboardInterrupt:
        print("\n[INFO] ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n[ERROR] í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
    finally:
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")