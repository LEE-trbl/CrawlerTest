from bs4 import BeautifulSoup as bs
from pathlib import Path
from openpyxl import Workbook
from fake_useragent import UserAgent
from requests.exceptions import RequestException, Timeout, ConnectTimeout, ReadTimeout
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.webdriver.common.proxy import Proxy, ProxyType
import time
import os
import re
import requests as rq
import math
import sys
import random
import itertools
import json
from urllib.parse import urlencode


class MobileUserAgent:
    """Android, Macintosh, iPhoneë§Œ ì‚¬ìš©í•˜ëŠ” User-Agent ìƒì„±ê¸°"""

    def __init__(self):
        self.user_agents = [
            # Android Chrome
            "Mozilla/5.0 (Linux; Android 13; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
            "Mozilla/5.0 (Linux; Android 12; SM-A525F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36",
            "Mozilla/5.0 (Linux; Android 11; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Mobile Safari/537.36",
            "Mozilla/5.0 (Linux; Android 13; Pixel 7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
            "Mozilla/5.0 (Linux; Android 12; Pixel 6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36",

            # iPhone Safari
            "Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1",
            "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1",
            "Mozilla/5.0 (iPhone; CPU iPhone OS 15_7 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6 Mobile/15E148 Safari/604.1",

            # iPhone Chrome
            "Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/120.0.6099.119 Mobile/15E148 Safari/604.1",
            "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/119.0.6045.169 Mobile/15E148 Safari/604.1",

            # iPad Safari
            "Mozilla/5.0 (iPad; CPU OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1",
            "Mozilla/5.0 (iPad; CPU OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1",

            # Macintosh Safari
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15",

            # Macintosh Chrome
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",

            # Macintosh Firefox
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/120.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/119.0",

            # M1/M2 Mac
            "Mozilla/5.0 (Macintosh; Apple M1 Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
            "Mozilla/5.0 (Macintosh; Apple M2 Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]

    @property
    def random(self):
        """ëœë¤ User-Agent ë°˜í™˜"""
        return random.choice(self.user_agents)

    def get_mobile_ua(self):
        """ëª¨ë°”ì¼ ì „ìš© User-Agent ë°˜í™˜"""
        mobile_uas = [ua for ua in self.user_agents if 'Mobile' in ua or 'iPhone' in ua or 'Android' in ua]
        return random.choice(mobile_uas)

    def get_desktop_ua(self):
        """ë°ìŠ¤í¬í†± ì „ìš© User-Agent ë°˜í™˜ (Macë§Œ)"""
        desktop_uas = [ua for ua in self.user_agents if 'Macintosh' in ua and 'Mobile' not in ua]
        return random.choice(desktop_uas)


class ProxyRotator:
    def __init__(self, proxy_list=None):
        """
        í”„ë¡ì‹œ ë¡œí…Œì´í„° ì´ˆê¸°í™”
        proxy_list: ['ip:port:username:password', ...] í˜•íƒœì˜ í”„ë¡ì‹œ ë¦¬ìŠ¤íŠ¸
        """
        self.proxy_list = proxy_list if proxy_list else []
        self.proxy_cycle = itertools.cycle(self.proxy_list) if self.proxy_list else None
        self.current_proxy = None
        self.failed_proxies = set()
        self.proxy_failure_count = {}  # í”„ë¡ì‹œë³„ ì‹¤íŒ¨ íšŸìˆ˜ ì¶”ì 
        self.max_failures_per_proxy = 3  # í”„ë¡ì‹œë‹¹ ìµœëŒ€ ì‹¤íŒ¨ í—ˆìš© íšŸìˆ˜

    def get_next_proxy(self):
        """ë‹¤ìŒ í”„ë¡ì‹œë¥¼ ë°˜í™˜"""
        if not self.proxy_cycle:
            return None

        # ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œë¥¼ ì°¾ì„ ë•Œê¹Œì§€ ìˆœí™˜
        attempts = 0
        max_attempts = len(self.proxy_list) * 2  # ë¬´í•œ ë£¨í”„ ë°©ì§€

        while attempts < max_attempts:
            proxy = next(self.proxy_cycle)

            # ì™„ì „íˆ ì‹¤íŒ¨í•œ í”„ë¡ì‹œê°€ ì•„ë‹ˆë¼ë©´ ì‚¬ìš©
            if proxy not in self.failed_proxies:
                self.current_proxy = proxy
                proxy_ip = proxy.split(':')[0]
                failure_count = self.proxy_failure_count.get(proxy, 0)
                print(f"[PROXY] í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ í”„ë¡ì‹œ: {proxy_ip} (ì‹¤íŒ¨ íšŸìˆ˜: {failure_count})")
                return proxy

            attempts += 1

        # ëª¨ë“  í”„ë¡ì‹œê°€ ì™„ì „íˆ ì‹¤íŒ¨í–ˆë‹¤ë©´ ì‹¤íŒ¨ ëª©ë¡ì„ ì´ˆê¸°í™”
        if len(self.failed_proxies) == len(self.proxy_list):
            print("[WARNING] ëª¨ë“  í”„ë¡ì‹œê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì‹¤íŒ¨ ëª©ë¡ê³¼ ì¹´ìš´í„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            self.failed_proxies.clear()
            self.proxy_failure_count.clear()

            # ì²« ë²ˆì§¸ í”„ë¡ì‹œ ë°˜í™˜
            if self.proxy_list:
                self.current_proxy = self.proxy_list[0]
                proxy_ip = self.current_proxy.split(':')[0]
                print(f"[PROXY] ì´ˆê¸°í™” í›„ ì‚¬ìš© ì¤‘ì¸ í”„ë¡ì‹œ: {proxy_ip}")
                return self.current_proxy

        return None

    def mark_proxy_failed(self, proxy):
        """í”„ë¡ì‹œë¥¼ ì‹¤íŒ¨ë¡œ í‘œì‹œ (ëˆ„ì  ì‹¤íŒ¨ ê´€ë¦¬)"""
        if proxy not in self.proxy_failure_count:
            self.proxy_failure_count[proxy] = 0

        self.proxy_failure_count[proxy] += 1
        proxy_ip = proxy.split(':')[0]

        # ìµœëŒ€ ì‹¤íŒ¨ íšŸìˆ˜ì— ë„ë‹¬í•˜ë©´ ì™„ì „íˆ ì œê±°
        if self.proxy_failure_count[proxy] >= self.max_failures_per_proxy:
            self.failed_proxies.add(proxy)
            print(f"[WARNING] í”„ë¡ì‹œ ì™„ì „ ì‹¤íŒ¨ë¡œ ì œê±°: {proxy_ip} ({self.proxy_failure_count[proxy]}íšŒ ì‹¤íŒ¨)")
        else:
            print(
                f"[WARNING] í”„ë¡ì‹œ ì¼ì‹œ ì‹¤íŒ¨: {proxy_ip} ({self.proxy_failure_count[proxy]}/{self.max_failures_per_proxy} ì‹¤íŒ¨)")

    def get_available_proxy_count(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œ ê°œìˆ˜ ë°˜í™˜"""
        if not self.proxy_list:
            return 0
        return len(self.proxy_list) - len(self.failed_proxies)

    def get_proxy_dict(self, proxy_string):
        """í”„ë¡ì‹œ ë¬¸ìì—´ì„ requestsìš© ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        if not proxy_string:
            return None

        parts = proxy_string.split(':')
        if len(parts) == 2:  # ip:port
            ip, port = parts
            return {
                'http': f'http://{ip}:{port}',
                'https': f'http://{ip}:{port}'
            }
        elif len(parts) == 4:  # ip:port:username:password
            ip, port, username, password = parts
            return {
                'http': f'http://{username}:{password}@{ip}:{port}',
                'https': f'http://{username}:{password}@{ip}:{port}'
            }
        return None


class ChromeDriver:
    def __init__(self, proxy_rotator=None) -> None:
        self.proxy_rotator = proxy_rotator
        self.ua = MobileUserAgent()  # ì»¤ìŠ¤í…€ ëª¨ë°”ì¼ User-Agent ì‚¬ìš©
        self.set_options()
        self.set_driver()

    def set_options(self) -> None:
        self.options = Options()
        self.options.add_argument("--headless")
        self.options.add_argument("lang=ko_KR")

        # ëª¨ë°”ì¼/Mac ì „ìš© User-Agent ì‚¬ìš©
        user_agent = self.ua.random
        self.options.add_argument(f"user-agent={user_agent}")
        print(f"[DEBUG] ì‚¬ìš© ì¤‘ì¸ User-Agent: {user_agent}")

        # ë” ë§ì€ ë¸Œë¼ìš°ì € ì˜µì…˜ ì¶”ê°€ë¡œ íƒì§€ ë°©ì§€
        self.options.add_argument("--log-level=3")
        self.options.add_argument("--no-sandbox")
        self.options.add_argument("--disable-dev-shm-usage")
        self.options.add_argument("--disable-blink-features=AutomationControlled")
        self.options.add_argument("--exclude-switches=enable-automation")
        self.options.add_argument("--disable-extensions")
        self.options.add_argument("--no-first-run")
        self.options.add_argument("--disable-default-apps")
        self.options.add_argument("--disable-infobars")
        self.options.add_experimental_option("detach", True)
        self.options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.options.add_experimental_option('useAutomationExtension', False)

        # í”„ë¡ì‹œ ì„¤ì •
        if self.proxy_rotator:
            proxy = self.proxy_rotator.get_next_proxy()
            if proxy:
                parts = proxy.split(':')
                if len(parts) >= 2:
                    ip, port = parts[0], parts[1]
                    self.options.add_argument(f'--proxy-server=http://{ip}:{port}')
                    print(f"[DEBUG] Selenium í”„ë¡ì‹œ ì„¤ì •: {ip}:{port}")

    def set_driver(self) -> None:
        self.driver = webdriver.Chrome(options=self.options)
        # WebDriver íƒì§€ ë°©ì§€
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    def refresh_with_new_proxy(self):
        """ìƒˆë¡œìš´ í”„ë¡ì‹œë¡œ ë“œë¼ì´ë²„ ì¬ì‹œì‘"""
        if hasattr(self, 'driver') and self.driver:
            self.driver.quit()
        self.set_options()
        self.set_driver()


class Coupang:
    @staticmethod
    def get_product_code(url: str) -> str:
        prod_code: str = url.split("products/")[-1].split("?")[0]
        return prod_code

    @staticmethod
    def get_soup_object(resp: rq.Response) -> bs:
        return bs(resp.text, "html.parser")

    def __del__(self) -> None:
        if hasattr(self, 'ch') and self.ch.driver:
            self.ch.driver.quit()

    def __init__(self, proxy_list=None) -> None:
        self.base_review_url: str = "https://www.coupang.com/vp/product/reviews"
        self.sd = SaveData()
        self.retries = 8  # ì¬ì‹œë„ íšŸìˆ˜ ì¤„ì„
        self.delay_min = 2.0  # ìµœì†Œ ë”œë ˆì´ ì¦ê°€
        self.delay_max = 8.0  # ìµœëŒ€ ë”œë ˆì´ ì¦ê°€
        self.page_delay_min = 3.0  # í˜ì´ì§€ ê°„ ìµœì†Œ ë”œë ˆì´ ì¦ê°€
        self.page_delay_max = 10.0  # í˜ì´ì§€ ê°„ ìµœëŒ€ ë”œë ˆì´ ì¦ê°€

        # íƒ€ì„ì•„ì›ƒ ê´€ë ¨ ì„¤ì •
        self.consecutive_timeouts = 0
        self.max_consecutive_timeouts = 3  # ì—°ì† íƒ€ì„ì•„ì›ƒ í—ˆìš© íšŸìˆ˜ ê°ì†Œ
        self.long_wait_min = 300  # ê¸´ ëŒ€ê¸° ì‹œê°„ ì¤„ì„ (5ë¶„)
        self.long_wait_max = 420  # ê¸´ ëŒ€ê¸° ì‹œê°„ ì¤„ì„ (7ë¶„)

        # í”„ë¡ì‹œ ë¡œí…Œì´í„° ì´ˆê¸°í™”
        self.proxy_rotator = ProxyRotator(proxy_list)

        # ëª¨ë°”ì¼/Mac ì „ìš© User-Agent ì´ˆê¸°í™”
        self.ua = MobileUserAgent()

        # ë” ì •êµí•œ í—¤ë” ì„¤ì •
        self.base_headers = {
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "cache-control": "max-age=0",
            "sec-ch-ua": '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"macOS"',  # Windows ëŒ€ì‹  macOS ì‚¬ìš©
            "sec-fetch-dest": "document",
            "sec-fetch-mode": "navigate",
            "sec-fetch-site": "same-origin",
            "sec-fetch-user": "?1",
            "upgrade-insecure-requests": "1",
            "dnt": "1",
        }

        # ì¿ í‚¤ ì €ì¥ìš© ì„¸ì…˜
        self.session = rq.Session()

        # í—¤ë”ì— ëœë¤ User-Agent ì ìš©
        self.update_headers()

        self.ch = ChromeDriver(self.proxy_rotator)
        self.page_title = None

    def get_realistic_headers(self):
        """ì‹¤ì œ ë¸Œë¼ìš°ì €ì™€ ìœ ì‚¬í•œ í—¤ë” ìƒì„± (ëª¨ë°”ì¼/Mac ì „ìš©)"""
        headers = self.base_headers.copy()
        user_agent = self.ua.random
        headers["user-agent"] = user_agent

        # User-Agentì— ë”°ë¼ í”Œë«í¼ ì •ë³´ ì¡°ì •
        if 'iPhone' in user_agent or 'iPad' in user_agent:
            headers["sec-ch-ua-platform"] = '"iOS"'
            headers["sec-ch-ua-mobile"] = "?1" if 'iPhone' in user_agent else "?0"
        elif 'Android' in user_agent:
            headers["sec-ch-ua-platform"] = '"Android"'
            headers["sec-ch-ua-mobile"] = "?1"
        elif 'Macintosh' in user_agent:
            headers["sec-ch-ua-platform"] = '"macOS"'
            headers["sec-ch-ua-mobile"] = "?0"

        # ëœë¤ ìš”ì†Œ ì¶”ê°€
        if random.choice([True, False]):
            headers["x-requested-with"] = "XMLHttpRequest"

        # ì¿ íŒ¡ íŠ¹í™” í—¤ë”
        headers.update({
            "x-coupang-target-market": "KR",
            "x-coupang-accept-language": "ko-KR",
        })

        return headers

    def update_headers(self):
        """í—¤ë”ë¥¼ ìƒˆë¡œìš´ User-Agentë¡œ ì—…ë°ì´íŠ¸"""
        self.headers = self.get_realistic_headers()
        print(f"[DEBUG] í—¤ë” User-Agent ì—…ë°ì´íŠ¸: {self.headers['user-agent'][:70]}...")

    def get_session_with_proxy(self):
        """í”„ë¡ì‹œê°€ ì ìš©ëœ requests ì„¸ì…˜ ë°˜í™˜"""
        session = rq.Session()
        session.headers.update(self.headers)

        # ë” í˜„ì‹¤ì ì¸ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        session.timeout = (10, 30)  # ì—°ê²° íƒ€ì„ì•„ì›ƒ 10ì´ˆ, ì½ê¸° íƒ€ì„ì•„ì›ƒ 30ì´ˆ

        if self.proxy_rotator and self.proxy_rotator.proxy_list:
            proxy = self.proxy_rotator.get_next_proxy()
            if proxy:
                proxy_dict = self.proxy_rotator.get_proxy_dict(proxy)
                if proxy_dict:
                    session.proxies.update(proxy_dict)
                    print(f"[DEBUG] ìš”ì²­ì— í”„ë¡ì‹œ ì ìš©: {proxy}")

        return session

    def warm_up_session(self, prod_code):
        """ì„¸ì…˜ì„ ì˜ˆì—´í•˜ì—¬ ì¿ íŒ¡ ì‚¬ì´íŠ¸ì™€ì˜ ì—°ê²°ì„ ì„¤ì •"""
        try:
            print("[INFO] ì„¸ì…˜ ì˜ˆì—´ ì¤‘...")

            # ë©”ì¸ í˜ì´ì§€ ë¨¼ì € ë°©ë¬¸
            main_url = "https://www.coupang.com"
            session = self.get_session_with_proxy()

            # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸
            resp = session.get(main_url, timeout=15)
            if resp.status_code == 200:
                print("[DEBUG] ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ ì„±ê³µ")

                # ì¿ í‚¤ ì—…ë°ì´íŠ¸
                self.session.cookies.update(resp.cookies)

                # ì ì‹œ ëŒ€ê¸°
                time.sleep(random.uniform(2, 4))

                # ìƒí’ˆ í˜ì´ì§€ ë°©ë¬¸
                product_url = f"https://www.coupang.com/vp/products/{prod_code}"
                resp2 = session.get(product_url, timeout=15)

                if resp2.status_code == 200:
                    print("[DEBUG] ìƒí’ˆ í˜ì´ì§€ ë°©ë¬¸ ì„±ê³µ")
                    self.session.cookies.update(resp2.cookies)
                    return True

        except Exception as e:
            print(f"[WARNING] ì„¸ì…˜ ì˜ˆì—´ ì‹¤íŒ¨: {e}")

        return False

    def get_product_title(self, prod_code: str) -> str:
        """ìƒí’ˆëª…ë§Œ ê°„ë‹¨í•˜ê²Œ ì¶”ì¶œ"""
        url = f"https://www.coupang.com/vp/products/{prod_code}"
        print(f"[DEBUG] ìƒí’ˆ í˜ì´ì§€ ì ‘ì† ì¤‘: {url}")

        try:
            # ì—¬ëŸ¬ ë²ˆ ì‹œë„
            for attempt in range(3):
                try:
                    self.ch.driver.get(url=url)

                    print("[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
                    WebDriverWait(self.ch.driver, 30).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )

                    loading_delay = random.uniform(3.0, 6.0)  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                    print(f"[DEBUG] {loading_delay:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(loading_delay)

                    page_source: str = self.ch.driver.page_source
                    soup = bs(page_source, "html.parser")

                    # ë” ë§ì€ ìƒí’ˆëª… ì„ íƒì ì‹œë„
                    title_selectors = [
                        "h1.prod-buy-header__title",
                        ".prod-buy-header__title",
                        "h1[class*='title']",
                        ".product-title",
                        "h1",
                        ".prod-title",
                        "[data-testid='product-title']",
                        ".product-name"
                    ]

                    for selector in title_selectors:
                        title_elem = soup.select_one(selector)
                        if title_elem and title_elem.text.strip():
                            title = title_elem.text.strip()
                            print(f"[DEBUG] ìƒí’ˆëª… ë°œê²¬: {title}")
                            return title

                    print(f"[WARNING] ì‹œë„ {attempt + 1}/3 - ìƒí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    if attempt < 2:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´
                        time.sleep(random.uniform(3, 5))

                except Exception as e:
                    print(f"[ERROR] ì‹œë„ {attempt + 1}/3 ì‹¤íŒ¨: {e}")
                    if attempt < 2:
                        time.sleep(random.uniform(3, 5))

            print("[WARNING] ëª¨ë“  ì‹œë„ í›„ì—ë„ ìƒí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "ìƒí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨"

        except Exception as e:
            print(f"[ERROR] get_product_title ì—ëŸ¬: {e}")
            return "ìƒí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨"

    def is_timeout_error(self, exception) -> bool:
        """íƒ€ì„ì•„ì›ƒ ê´€ë ¨ ì˜ˆì™¸ì¸ì§€ í™•ì¸"""
        return isinstance(exception, (Timeout, ConnectTimeout, ReadTimeout)) or \
            (isinstance(exception, RequestException) and "timeout" in str(exception).lower())

    def handle_consecutive_timeouts(self) -> None:
        """ì—°ì† íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬"""
        if self.consecutive_timeouts >= self.max_consecutive_timeouts:
            wait_time = random.uniform(self.long_wait_min, self.long_wait_max)
            wait_minutes = wait_time / 60
            print(f"[WARNING] ì—°ì† {self.consecutive_timeouts}íšŒ íƒ€ì„ì•„ì›ƒ ë°œìƒ!")
            print(f"[INFO] ì„œë²„ ì•ˆì •í™”ë¥¼ ìœ„í•´ {wait_minutes:.1f}ë¶„ ëŒ€ê¸°í•©ë‹ˆë‹¤...")

            remaining_time = wait_time
            while remaining_time > 0:
                minutes_left = remaining_time / 60
                print(f"[INFO] ë‚¨ì€ ëŒ€ê¸° ì‹œê°„: {minutes_left:.1f}ë¶„")

                sleep_duration = min(30, remaining_time)
                time.sleep(sleep_duration)
                remaining_time -= sleep_duration

            print(f"[INFO] ëŒ€ê¸° ì™„ë£Œ! í¬ë¡¤ë§ì„ ì¬ê°œí•©ë‹ˆë‹¤.")
            self.consecutive_timeouts = 0

    def start(self) -> None:
        self.sd.create_directory()
        URL: str = self.input_review_url()

        if '#' in URL:
            URL = URL.split('#')[0]
            print(f"[DEBUG] URL fragment ì œê±°: {URL}")

        prod_code: str = self.get_product_code(url=URL)
        print(f"[DEBUG] ìƒí’ˆ ì½”ë“œ: {prod_code}")

        # ì„¸ì…˜ ì˜ˆì—´
        self.warm_up_session(prod_code)

        try:
            self.title = self.get_product_title(prod_code=prod_code)
            print(f"[INFO] ìƒí’ˆëª…: {self.title}")
        except Exception as e:
            print(f"[ERROR] ìƒí’ˆëª…ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            self.title = "ìƒí’ˆëª… ë¯¸í™•ì¸"

        print(f"[INFO] ëª¨ë“  ë¦¬ë·° í˜ì´ì§€ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")

        # í”„ë¡ì‹œ ì‚¬ìš© ì¤‘ì´ë¼ë©´ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œ ìˆ˜ ì¶œë ¥
        if self.proxy_rotator and self.proxy_rotator.proxy_list:
            available_proxies = self.proxy_rotator.get_available_proxy_count()
            print(f"[INFO] ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œ: {available_proxies}/{len(self.proxy_rotator.proxy_list)}ê°œ")

        success_count = 0
        current_page = 1
        consecutive_empty_pages = 0
        max_empty_pages = 5  # ì—°ì† ë¹ˆ í˜ì´ì§€ í—ˆìš© íšŸìˆ˜ ì¦ê°€
        proxy_change_attempts = 0  # í”„ë¡ì‹œ êµì²´ ì‹œë„ íšŸìˆ˜

        while consecutive_empty_pages < max_empty_pages:
            payload = {
                "productId": prod_code,
                "page": current_page,
                "size": 5,
                "sortBy": "ORDER_SCORE_ASC",
                "ratings": "",
                "q": "",
                "viRoleCode": 2,
                "ratingSummary": True,
            }

            result = self.fetch(payload=payload)

            if result:
                success_count += 1
                consecutive_empty_pages = 0
                proxy_change_attempts = 0  # ì„±ê³µí•˜ë©´ í”„ë¡ì‹œ êµì²´ ì‹œë„ íšŸìˆ˜ ë¦¬ì…‹
            else:
                consecutive_empty_pages += 1
                print(f"[WARNING] í˜ì´ì§€ {current_page}ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({consecutive_empty_pages}/{max_empty_pages})")

                # ì—°ì† ë¹ˆ í˜ì´ì§€ê°€ 2ê°œ ì´ìƒì´ê³  í”„ë¡ì‹œë¥¼ ì‚¬ìš© ì¤‘ì´ë¼ë©´ í”„ë¡ì‹œ ìƒíƒœ ì²´í¬
                if (consecutive_empty_pages >= 2 and
                        self.proxy_rotator and
                        self.proxy_rotator.current_proxy and
                        proxy_change_attempts < 3):  # ìµœëŒ€ 3ë²ˆê¹Œì§€ í”„ë¡ì‹œ êµì²´ ì‹œë„

                    available_proxies = self.proxy_rotator.get_available_proxy_count()
                    if available_proxies > 1:  # ë‹¤ë¥¸ í”„ë¡ì‹œê°€ ìˆë‹¤ë©´
                        print(f"[INFO] ì—°ì† ì‹¤íŒ¨ë¡œ ì¸í•œ í”„ë¡ì‹œ êµì²´ ì‹œë„ ({proxy_change_attempts + 1}/3)")

                        # í˜„ì¬ í”„ë¡ì‹œë¥¼ ì¼ì‹œ ì‹¤íŒ¨ë¡œ í‘œì‹œ (ì™„ì „ ì œê±°ëŠ” ì•„ë‹˜)
                        self.proxy_rotator.mark_proxy_failed(self.proxy_rotator.current_proxy)
                        proxy_change_attempts += 1

                        # í”„ë¡ì‹œ êµì²´ í›„ í˜„ì¬ í˜ì´ì§€ ì¬ì‹œë„
                        print(f"[INFO] í˜ì´ì§€ {current_page} ë‹¤ë¥¸ í”„ë¡ì‹œë¡œ ì¬ì‹œë„...")
                        continue  # current_page ì¦ê°€ ì—†ì´ ì¬ì‹œë„

            current_page += 1

            if result and consecutive_empty_pages == 0:
                short_delay = random.uniform(1.0, 3.0)  # ì§§ì€ ë”œë ˆì´ë„ ì¦ê°€
                time.sleep(short_delay)

            if current_page > 500:  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ê°ì†Œ
                print("[INFO] ìµœëŒ€ í˜ì´ì§€ ìˆ˜(500)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                break

        print(f"[INFO] ì´ {success_count}ê°œ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ (ì´ {current_page - 1}í˜ì´ì§€ ì‹œë„)")

        # ìµœì¢… í”„ë¡ì‹œ ìƒíƒœ ì¶œë ¥
        if self.proxy_rotator and self.proxy_rotator.proxy_list:
            available_proxies = self.proxy_rotator.get_available_proxy_count()
            failed_proxies = len(self.proxy_rotator.failed_proxies)
            print(f"[INFO] ìµœì¢… í”„ë¡ì‹œ ìƒíƒœ: ì‚¬ìš© ê°€ëŠ¥ {available_proxies}ê°œ, ì™„ì „ ì‹¤íŒ¨ {failed_proxies}ê°œ")

    def fetch(self, payload: dict) -> bool:
        now_page: int = payload["page"]
        print(f"\n[INFO] Start crawling page {now_page} ...")
        attempt: int = 0
        proxy_attempts: int = 0  # í”„ë¡ì‹œ êµì²´ íšŸìˆ˜ ì¶”ì 
        max_proxy_attempts: int = min(10,
                                      len(self.proxy_rotator.proxy_list) if self.proxy_rotator else 0)  # ìµœëŒ€ í”„ë¡ì‹œ ì‹œë„ íšŸìˆ˜

        while attempt < self.retries:
            try:
                # ë§¤ ìš”ì²­ë§ˆë‹¤ ìƒˆë¡œìš´ User-Agent ì‚¬ìš©
                if attempt > 0:  # ì¬ì‹œë„ ì‹œì—ë§Œ User-Agent ë³€ê²½
                    self.update_headers()

                session = self.get_session_with_proxy()

                # ì„¸ì…˜ì— ê¸°ì¡´ ì¿ í‚¤ ì ìš©
                session.cookies.update(self.session.cookies)

                # Referer í—¤ë” ì¶”ê°€
                session.headers.update({
                    "Referer": f"https://www.coupang.com/vp/products/{payload['productId']}"
                })

                resp = session.get(
                    url=self.base_review_url,
                    params=payload,
                    timeout=(15, 30),  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
                )

                self.consecutive_timeouts = 0

                if resp.status_code == 403:
                    print(f"[ERROR] HTTP 403 ì‘ë‹µ - í”„ë¡ì‹œê°€ ì°¨ë‹¨ë¨")
                    if self.proxy_rotator and self.proxy_rotator.current_proxy:
                        self.proxy_rotator.mark_proxy_failed(self.proxy_rotator.current_proxy)
                    attempt += 1
                    continue
                elif resp.status_code != 200:
                    print(f"[ERROR] HTTP {resp.status_code} ì‘ë‹µ")
                    attempt += 1
                    continue

                html = resp.text
                soup = bs(html, "html.parser")

                # ë””ë²„ê¹…: ì‘ë‹µ ë‚´ìš© í™•ì¸
                if now_page == 1 and len(html) < 10000:
                    print(f"[DEBUG] ì‘ë‹µ HTML ê¸¸ì´: {len(html)}")
                    print(f"[DEBUG] ì‘ë‹µ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {html[:500]}")

                if self.page_title is None:
                    first_review = soup.select_one("article.sdp-review__article__list")
                    if first_review:
                        title_elem = first_review.select_one("div.sdp-review__article__list__info__product-info__name")
                        self.page_title = title_elem.text.strip() if title_elem else self.title
                    else:
                        self.page_title = self.title

                articles = soup.select("article.sdp-review__article__list")
                article_length = len(articles)

                if article_length == 0:
                    print(f"[WARNING] í˜ì´ì§€ {now_page}ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                    # í”„ë¡ì‹œ ì‚¬ìš© ì¤‘ì´ë¼ë©´ ë‹¤ë¥¸ í”„ë¡ì‹œë¡œ ì¬ì‹œë„
                    if self.proxy_rotator and self.proxy_rotator.current_proxy and proxy_attempts < max_proxy_attempts:
                        print(f"[INFO] í”„ë¡ì‹œ ì°¨ë‹¨ ê°€ëŠ¥ì„±ìœ¼ë¡œ ë‹¤ë¥¸ í”„ë¡ì‹œë¡œ ì¬ì‹œë„ ({proxy_attempts + 1}/{max_proxy_attempts})")

                        # í˜„ì¬ í”„ë¡ì‹œë¥¼ ì‹¤íŒ¨ë¡œ í‘œì‹œ
                        self.proxy_rotator.mark_proxy_failed(self.proxy_rotator.current_proxy)
                        proxy_attempts += 1
                        attempt += 1

                        # ì§§ì€ ë”œë ˆì´ í›„ ì¬ì‹œë„
                        retry_delay = random.uniform(1.0, 3.0)
                        print(f"[DEBUG] {retry_delay:.1f}ì´ˆ í›„ ë‹¤ë¥¸ í”„ë¡ì‹œë¡œ ì¬ì‹œë„...")
                        time.sleep(retry_delay)
                        continue

                    # ë” ì´ìƒ ì‹œë„í•  í”„ë¡ì‹œê°€ ì—†ê±°ë‚˜ í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°
                    if now_page == 1:
                        print("[DEBUG] ì²« í˜ì´ì§€ HTML êµ¬ì¡° í™•ì¸:")
                        print(f"  - ì „ì²´ ê¸¸ì´: {len(html)} ë¬¸ì")
                        print(f"  - 'review' í¬í•¨ íšŸìˆ˜: {html.lower().count('review')}")
                        print(f"  - 'article' í¬í•¨ íšŸìˆ˜: {html.lower().count('article')}")

                        # ì°¨ë‹¨ ì—¬ë¶€ í™•ì¸
                        blocked_indicators = [
                            "access denied", "blocked", "forbidden",
                            "captcha", "robot", "bot", "security", "verification"
                        ]

                        html_lower = html.lower()
                        is_blocked = False
                        for indicator in blocked_indicators:
                            if indicator in html_lower:
                                print(f"[WARNING] ì°¨ë‹¨ ê°ì§€: '{indicator}' ë°œê²¬")
                                is_blocked = True
                                break

                        # ì°¨ë‹¨ì´ ê°ì§€ë˜ë©´ ì¶”ê°€ ì¬ì‹œë„
                        if is_blocked and attempt < self.retries - 2:
                            print("[INFO] ì°¨ë‹¨ ê°ì§€ë¡œ ì¸í•œ ì¶”ê°€ ì¬ì‹œë„...")
                            attempt += 1
                            long_delay = random.uniform(5.0, 10.0)
                            print(f"[DEBUG] {long_delay:.1f}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                            time.sleep(long_delay)
                            continue

                        alt_selectors = [
                            "article[class*='review']",
                            "div[class*='review-item']",
                            "div[class*='review-list']",
                            ".review-item",
                            "[data-review-id]"
                        ]

                        for selector in alt_selectors:
                            elements = soup.select(selector)
                            if elements:
                                print(f"[DEBUG] ëŒ€ì•ˆ ì„ íƒì '{selector}' ë°œê²¬: {len(elements)}ê°œ")

                    return False

                print(f"[SUCCESS] í˜ì´ì§€ {now_page}ì—ì„œ {article_length}ê°œ ë¦¬ë·° ë°œê²¬")

                # ë¦¬ë·° ë°ì´í„° ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
                for idx in range(article_length):
                    dict_data: dict[str, str | int] = dict()

                    review_date_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__reg-date"
                    )
                    review_date = review_date_elem.text.strip() if review_date_elem else "-"

                    user_name_elem = articles[idx].select_one(
                        "span.sdp-review__article__list__info__user__name"
                    )
                    user_name = user_name_elem.text.strip() if user_name_elem else "-"

                    rating_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__star-orange"
                    )
                    if rating_elem and rating_elem.get("data-rating"):
                        try:
                            rating = int(rating_elem.get("data-rating"))
                        except (ValueError, TypeError):
                            rating = 0
                    else:
                        rating = 0

                    prod_name_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__name"
                    )
                    prod_name = prod_name_elem.text.strip() if prod_name_elem else "-"

                    headline_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__headline"
                    )
                    headline = headline_elem.text.strip() if headline_elem else "ë“±ë¡ëœ í—¤ë“œë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤"

                    review_content_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__review__content.js_reviewArticleContent"
                    )
                    if review_content_elem:
                        review_content = re.sub("[\n\t]", "", review_content_elem.text.strip())
                    else:
                        review_content_elem = articles[idx].select_one(
                            "div.sdp-review__article__list__review > div"
                        )
                        if review_content_elem:
                            review_content = re.sub("[\n\t]", "", review_content_elem.text.strip())
                        else:
                            review_content = "ë“±ë¡ëœ ë¦¬ë·°ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"

                    answer_elem = articles[idx].select_one(
                        "span.sdp-review__article__list__survey__row__answer"
                    )
                    answer = answer_elem.text.strip() if answer_elem else "ë§› í‰ê°€ ì—†ìŒ"

                    helpful_count_elem = articles[idx].select_one("span.js_reviewArticleHelpfulCount")
                    helpful_count = helpful_count_elem.text.strip() if helpful_count_elem else "0"

                    seller_name_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__seller_name"
                    )
                    if seller_name_elem:
                        seller_name = seller_name_elem.text.replace("íŒë§¤ì: ", "").strip()
                    else:
                        seller_name = "-"

                    review_images = articles[idx].select("div.sdp-review__article__list__attachment__list img")
                    image_count = len(review_images)

                    dict_data["title"] = self.page_title
                    dict_data["prod_name"] = prod_name
                    dict_data["review_date"] = review_date
                    dict_data["user_name"] = user_name
                    dict_data["rating"] = rating
                    dict_data["headline"] = headline
                    dict_data["review_content"] = review_content
                    dict_data["answer"] = answer
                    dict_data["helpful_count"] = helpful_count
                    dict_data["seller_name"] = seller_name
                    dict_data["image_count"] = image_count

                    self.sd.save(datas=dict_data)
                    print(f"[SUCCESS] ë¦¬ë·° ì €ì¥ ì™„ë£Œ: {user_name} - {rating}ì ")

                page_delay = random.uniform(self.page_delay_min, self.page_delay_max)
                print(f"[DEBUG] ë‹¤ìŒ í˜ì´ì§€ê¹Œì§€ {page_delay:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(page_delay)
                return True

            except RequestException as e:
                attempt += 1

                # í”„ë¡ì‹œ ê´€ë ¨ ì˜¤ë¥˜ì¸ì§€ í™•ì¸í•˜ê³  ì²˜ë¦¬
                error_str = str(e).lower()
                is_proxy_error = any(keyword in error_str for keyword in [
                    "403", "proxy", "connection", "timeout", "refused", "unreachable"
                ])

                if is_proxy_error and self.proxy_rotator and self.proxy_rotator.current_proxy:
                    self.proxy_rotator.mark_proxy_failed(self.proxy_rotator.current_proxy)
                    print("[INFO] í”„ë¡ì‹œ ì˜¤ë¥˜ë¡œ ì¸í•œ ë‹¤ë¥¸ í”„ë¡ì‹œë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")

                    # ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œê°€ ìˆëŠ”ì§€ í™•ì¸
                    available_proxies = self.proxy_rotator.get_available_proxy_count()
                    if available_proxies > 0:
                        print(f"[INFO] ë‚¨ì€ ì‚¬ìš© ê°€ëŠ¥ í”„ë¡ì‹œ: {available_proxies}ê°œ")
                    else:
                        print("[WARNING] ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")

                if self.is_timeout_error(e):
                    self.consecutive_timeouts += 1
                    print(f"[ERROR] íƒ€ì„ì•„ì›ƒ ë°œìƒ (ì—°ì† {self.consecutive_timeouts}íšŒ): {e}")

                    if self.consecutive_timeouts >= self.max_consecutive_timeouts:
                        self.handle_consecutive_timeouts()
                else:
                    self.consecutive_timeouts = 0
                    print(f"[ERROR] ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")

                print(f"[ERROR] Attempt {attempt}/{self.retries} failed")
                if attempt < self.retries:
                    retry_delay = random.uniform(self.delay_min, self.delay_max)
                    print(f"[DEBUG] {retry_delay:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(retry_delay)
                else:
                    print(f"[ERROR] ìµœëŒ€ ìš”ì²­ íšŸìˆ˜ ì´ˆê³¼! í˜ì´ì§€ {now_page} í¬ë¡¤ë§ ì‹¤íŒ¨.")
                    return False
            except Exception as e:
                print(f"[ERROR] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                self.consecutive_timeouts = 0
                return False

        return False

    @staticmethod
    def clear_console() -> None:
        command: str = "clear"
        if os.name in ("nt", "dos"):
            command = "cls"
        try:
            # TERM í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°ë¥¼ ì²˜ë¦¬
            if os.environ.get('TERM') is None:
                os.environ['TERM'] = 'xterm'
            os.system(command=command)
        except:
            pass  # ëª¨ë“  ì˜¤ë¥˜ ë¬´ì‹œ

    def input_review_url(self) -> str:
        while True:
            try:
                self.clear_console()
            except:
                pass

            review_url: str = input(
                "ì›í•˜ì‹œëŠ” ìƒí’ˆì˜ URL ì£¼ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”\n\n"
                "Ex)\n"
                "https://www.coupang.com/vp/products/7335597976?itemId=18741704367&vendorItemId=85873964906\n\n"
                "URL: "
            )
            if not review_url.strip():
                print("[ERROR] URL ì£¼ì†Œê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                time.sleep(2)
                continue

            if "coupang.com" not in review_url:
                print("[ERROR] ì˜¬ë°”ë¥¸ ì¿ íŒ¡ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”")
                time.sleep(2)
                continue

            return review_url.strip()


class SaveData:
    def __init__(self) -> None:
        self.wb: Workbook = Workbook()
        self.ws = self.wb.active
        self.ws.append([
            "ìƒí’ˆëª…", "êµ¬ë§¤ìƒí’ˆëª…", "ì‘ì„±ì¼ì", "êµ¬ë§¤ìëª…", "í‰ì ",
            "í—¤ë“œë¼ì¸", "ë¦¬ë·°ë‚´ìš©", "ë§›ë§Œì¡±ë„", "ë„ì›€ìˆ˜", "íŒë§¤ì", "ì´ë¯¸ì§€ìˆ˜"
        ])
        self.row: int = 2
        self.dir_name: str = "Coupang-reviews"
        self.create_directory()

    def create_directory(self) -> None:
        if not os.path.exists(self.dir_name):
            os.makedirs(self.dir_name)
            print(f"[INFO] ë””ë ‰í† ë¦¬ ìƒì„±: {self.dir_name}")

    def save(self, datas: dict[str, str | int]) -> None:
        try:
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', datas["title"])
            file_name: str = os.path.join(self.dir_name, safe_title + ".xlsx")

            self.ws[f"A{self.row}"] = datas["title"]
            self.ws[f"B{self.row}"] = datas["prod_name"]
            self.ws[f"C{self.row}"] = datas["review_date"]
            self.ws[f"D{self.row}"] = datas["user_name"]
            self.ws[f"E{self.row}"] = datas["rating"]
            self.ws[f"F{self.row}"] = datas["headline"]
            self.ws[f"G{self.row}"] = datas["review_content"]
            self.ws[f"H{self.row}"] = datas["answer"]
            self.ws[f"I{self.row}"] = datas["helpful_count"]
            self.ws[f"J{self.row}"] = datas["seller_name"]
            self.ws[f"K{self.row}"] = datas["image_count"]

            self.row += 1
            self.wb.save(filename=file_name)

        except Exception as e:
            print(f"[ERROR] ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def __del__(self) -> None:
        try:
            if hasattr(self, 'wb'):
                self.wb.close()
        except:
            pass


def test_proxy(proxy_string):
    """í”„ë¡ì‹œ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        parts = proxy_string.split(':')
        if len(parts) == 4:
            ip, port, username, password = parts
            proxy_dict = {
                'http': f'http://{username}:{password}@{ip}:{port}',
                'https': f'http://{username}:{password}@{ip}:{port}'
            }
        else:
            return False

        response = rq.get('http://httpbin.org/ip', proxies=proxy_dict, timeout=10)
        if response.status_code == 200:
            return True
    except:
        pass
    return False


def get_proxy_list():
    """í”„ë¡ì‹œ ëª©ë¡ ë°˜í™˜"""
    # ì œê³µë°›ì€ í”„ë¡ì‹œ ëª©ë¡
    proxy_list = [
        "198.23.239.134:6540:bwedbcvt:3ocdjeqvcfpi",
        "207.244.217.165:6712:bwedbcvt:3ocdjeqvcfpi",
        "107.172.163.27:6543:bwedbcvt:3ocdjeqvcfpi",
        "161.123.152.115:6360:bwedbcvt:3ocdjeqvcfpi",
        "23.94.138.75:6349:bwedbcvt:3ocdjeqvcfpi",
        "216.10.27.159:6837:bwedbcvt:3ocdjeqvcfpi",
        "136.0.207.84:6661:bwedbcvt:3ocdjeqvcfpi",
        "64.64.118.149:6732:bwedbcvt:3ocdjeqvcfpi",
        "142.147.128.93:6593:bwedbcvt:3ocdjeqvcfpi",
        "154.36.110.199:6853:bwedbcvt:3ocdjeqvcfpi"
    ]

    print(f"[INFO] {len(proxy_list)}ê°œì˜ í”„ë¡ì‹œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("[NOTICE] ê°œì„ ëœ í”„ë¡ì‹œ ë¡œí…Œì´ì…˜ ì‹œìŠ¤í…œ:")
    print("  âœ… ì§€ëŠ¥ì  í”„ë¡ì‹œ êµì²´ (ì‹¤íŒ¨ ì‹œ ìë™ ì „í™˜)")
    print("  âœ… ëˆ„ì  ì‹¤íŒ¨ ê´€ë¦¬ (3íšŒ ì‹¤íŒ¨ ì‹œ ì™„ì „ ì œê±°)")
    print("  âœ… ë¦¬ë·° ì—†ìŒ = í”„ë¡ì‹œ ì°¨ë‹¨ìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ì¬ì‹œë„")
    print("  âœ… ì—°ì† ë¹ˆ í˜ì´ì§€ ì‹œ í”„ë¡ì‹œ êµì²´")
    print("  âœ… Android/iPhone/Mac ì „ìš© User-Agent ì‚¬ìš© (Windows ì œì™¸)")
    print()
    print("[NOTICE] HTTP 403/ì°¨ë‹¨ ë°œìƒ ì‹œ:")
    print("  1. ìë™ìœ¼ë¡œ ë‹¤ë¥¸ í”„ë¡ì‹œë¡œ êµì²´")
    print("  2. í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰ (n ì„ íƒ)")
    print("  3. VPN ì‚¬ìš© ê¶Œì¥")
    print()

    # í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_proxy = input("í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").lower().strip()

    if use_proxy == 'n':
        print("[INFO] í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return None
    else:
        print("[INFO] í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.")

        # í”„ë¡ì‹œ í…ŒìŠ¤íŠ¸ ì—¬ë¶€ í™•ì¸
        test_proxies = input("í”„ë¡ì‹œ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower().strip()

        if test_proxies == 'y':
            print("\n[INFO] í”„ë¡ì‹œ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
            working_proxies = []

            for i, proxy in enumerate(proxy_list, 1):
                print(f"[TEST] {i}/{len(proxy_list)} - {proxy.split(':')[0]}:{proxy.split(':')[1]} í…ŒìŠ¤íŠ¸ ì¤‘...", end='')
                if test_proxy(proxy):
                    print(" âœ… ì„±ê³µ")
                    working_proxies.append(proxy)
                else:
                    print(" âŒ ì‹¤íŒ¨")

            if working_proxies:
                print(f"\n[SUCCESS] {len(working_proxies)}/{len(proxy_list)}ê°œ í”„ë¡ì‹œê°€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
                print(f"[INFO] ì‘ë™í•˜ëŠ” í”„ë¡ì‹œë§Œ ì‚¬ìš©í•˜ì—¬ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                return working_proxies
            else:
                print("\n[ERROR] ì‘ë™í•˜ëŠ” í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
                fallback = input("í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").lower().strip()
                if fallback != 'n':
                    print("[INFO] í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    return None
                else:
                    print("[INFO] í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    exit(0)
        else:
            print("[INFO] í…ŒìŠ¤íŠ¸ ì—†ì´ ëª¨ë“  í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            print("[INFO] ì‹¤í–‰ ì¤‘ ìë™ìœ¼ë¡œ ì‘ë™í•˜ì§€ ì•ŠëŠ” í”„ë¡ì‹œë¥¼ ì œì™¸í•©ë‹ˆë‹¤.")
            return proxy_list


if __name__ == "__main__":
    try:
        print("=" * 70)
        print("ğŸ›’ ì¿ íŒ¡ ë¦¬ë·° í¬ë¡¤ëŸ¬ v3.5 (ëª¨ë°”ì¼/Mac ì „ìš© User-Agent)")
        print("=" * 70)

        # í”„ë¡ì‹œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        proxy_list = get_proxy_list()

        coupang = Coupang(proxy_list=proxy_list)
        coupang.start()

        print("\n" + "=" * 70)
        print("âœ… í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ“ ê²°ê³¼ íŒŒì¼ì€ 'Coupang-reviews' í´ë”ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
        print("=" * 70)

    except KeyboardInterrupt:
        print("\n[INFO] ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n[ERROR] í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
    finally:
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")