#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¿ íŒ¡ ê³°ê³° ë¸Œëœë“œ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
ë™ì  í˜ì´ì§€ ì²˜ë¦¬ ë° ëŒ€ìš©ëŸ‰ ë°ì´í„° ìˆ˜ì§‘
"""

import csv
import json
import logging
import random
import re
import time
import sys
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from urllib.parse import urljoin, urlparse, parse_qs

import requests
from bs4 import BeautifulSoup, Tag

from selenium import webdriver
from selenium.webdriver.chrome.options import ChromiumOptions
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import TimeoutException, NoSuchElementException

SELENIUM_AVAILABLE = True


# =================== ì„¤ì • ë° ë°ì´í„° í´ë˜ìŠ¤ ===================
@dataclass
class CrawlingConfig:
    """í¬ë¡¤ë§ ì„¤ì •"""
    base_url: str = "https://www.coupang.com"
    brand_url: str = "https://www.coupang.com/np/products/brand-shop"
    brand_name: str = "í™ˆí”Œë˜ë‹›"
    max_pages: int = 5
    delay_range: tuple = (2.0, 5.0)
    max_retries: int = 3
    timeout: int = 30

    # ì¶œë ¥ ì„¤ì •
    output_dir: str = "./coupang_í™ˆí”Œë˜ë‹›_data"
    csv_filename: str = "í™ˆí”Œë˜ë‹›_products_{timestamp}.csv"
    json_filename: str = "í™ˆí”Œë˜ë‹›_products_{timestamp}.json"

    # ë¡œê¹… ì„¤ì •
    log_level: str = "INFO"

    # Selenium ì„¤ì •
    headless: bool = True
    window_size: tuple = (1920, 1080)

    # User-Agent ëª©ë¡
    user_agents: List[str] = None

    def __post_init__(self):
        if self.user_agents is None:
            self.user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            ]


@dataclass
class ProductData:
    """ìƒí’ˆ ë°ì´í„° êµ¬ì¡°"""
    product_id: str
    product_name: str
    price: str
    original_price: str
    discount_rate: str
    unit_price: str
    rating: str
    review_count: str
    product_url: str
    image_url: str
    delivery_info: str
    cashback_amount: str
    is_rocket_delivery: bool
    vendor_item_id: str
    item_id: str
    page_number: int
    crawled_at: str


# =================== ë¡œê¹… ì„¤ì • ===================
class LoggerManager:
    """ë¡œê¹… ê´€ë¦¬ í´ë˜ìŠ¤"""

    @staticmethod
    def setup_logger(config: CrawlingConfig) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger('coupang_live_crawler')
        logger.setLevel(getattr(logging, config.log_level))

        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)

        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)

        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        log_dir = Path(config.output_dir) / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)

        # íŒŒì¼ í•¸ë“¤ëŸ¬
        file_handler = logging.FileHandler(
            log_dir / f"crawler_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
            encoding='utf-8'
        )
        file_handler.setLevel(logging.DEBUG)

        # í¬ë§·í„°
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)

        logger.addHandler(console_handler)
        logger.addHandler(file_handler)

        return logger


# =================== ì„¸ì…˜ ê´€ë¦¬ í´ë˜ìŠ¤ ===================
class SessionManager:
    """HTTP ì„¸ì…˜ ê´€ë¦¬"""

    def __init__(self, config: CrawlingConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.session = self._create_session()

    def _create_session(self) -> requests.Session:
        """ì„¸ì…˜ ìƒì„±"""
        session = requests.Session()

        # ê¸°ë³¸ í—¤ë” ì„¤ì •
        session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'User-Agent': random.choice(self.config.user_agents)
        })

        return session

    def get_with_retry(self, url: str, **kwargs) -> Optional[requests.Response]:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ GET ìš”ì²­"""
        for attempt in range(self.config.max_retries):
            try:
                # User-Agent ë¡œí…Œì´ì…˜
                self.session.headers['User-Agent'] = random.choice(self.config.user_agents)

                response = self.session.get(
                    url,
                    timeout=self.config.timeout,
                    **kwargs
                )

                if response.status_code == 200:
                    return response
                else:
                    self.logger.warning(f"HTTP {response.status_code}: {url}")

            except requests.RequestException as e:
                self.logger.warning(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.config.max_retries}): {e}")
                if attempt < self.config.max_retries - 1:
                    time.sleep(random.uniform(1, 3))

        return None


# =================== Selenium ë“œë¼ì´ë²„ ê´€ë¦¬ ===================
class SeleniumDriverManager:
    """Selenium ë“œë¼ì´ë²„ ê´€ë¦¬"""

    def __init__(self, config: CrawlingConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.driver = None
        self.wait = None

        if not SELENIUM_AVAILABLE:
            raise ImportError("Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install seleniumìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        try:
            options = ChromiumOptions()

            # ê¸°ë³¸ ì˜µì…˜
            if self.config.headless:
                options.add_argument('--headless')

            options.add_argument(f'--window-size={self.config.window_size[0]},{self.config.window_size[1]}')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-blink-features=AutomationControlled')

            # ë””í…ì…˜ ë°©ì§€
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)

            # ë¦¬ì†ŒìŠ¤ ì ˆì•½
            prefs = {
                "profile.managed_default_content_settings.images": 2,  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”
                "profile.default_content_setting_values.notifications": 2,
                "profile.managed_default_content_settings.plugins": 2,
            }
            options.add_experimental_option("prefs", prefs)

            # User-Agent ì„¤ì •
            options.add_argument(f'--user-agent={random.choice(self.config.user_agents)}')

            self.driver = webdriver.Chrome(options=options)

            # WebDriver íƒì§€ ë°©ì§€ ìŠ¤í¬ë¦½íŠ¸
            self.driver.execute_script(
                "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
            )

            self.wait = WebDriverWait(self.driver, self.config.timeout)

            self.logger.info("Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    def navigate_to_page(self, url: str, page: int = 1) -> bool:
        """í˜ì´ì§€ ì´ë™"""
        try:
            full_url = f"{url}?brandName={self.config.brand_name}&page={page}"
            self.logger.info(f"í˜ì´ì§€ ì´ë™: {full_url}")

            self.driver.get(full_url)

            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            self.wait.until(
                EC.presence_of_element_located((By.ID, "productList"))
            )

            # ì¶”ê°€ ë¡œë”© ëŒ€ê¸°
            time.sleep(random.uniform(*self.config.delay_range))

            return True

        except TimeoutException:
            self.logger.error(f"í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {url}")
            return False
        except Exception as e:
            self.logger.error(f"í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
            return False

    def scroll_and_load_content(self) -> str:
        """ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ"""
        self.logger.info("í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì‹œì‘")

        last_height = self.driver.execute_script("return document.body.scrollHeight")
        no_change_count = 0

        while no_change_count < 3:
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

            # ë¡œë”© ëŒ€ê¸°
            time.sleep(random.uniform(2, 4))

            # ìƒˆë¡œìš´ ë†’ì´ í™•ì¸
            new_height = self.driver.execute_script("return document.body.scrollHeight")

            if new_height == last_height:
                no_change_count += 1
            else:
                no_change_count = 0
                last_height = new_height

        # ìµœì¢… í˜ì´ì§€ ì†ŒìŠ¤ ë°˜í™˜
        return self.driver.page_source

    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            self.logger.info("ë“œë¼ì´ë²„ ì¢…ë£Œ")


# =================== ë°ì´í„° ì¶”ì¶œ í´ë˜ìŠ¤ ===================
class CoupangDataExtractor:
    """ì¿ íŒ¡ ë°ì´í„° ì¶”ì¶œ"""

    def __init__(self, config: CrawlingConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger

    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text.strip())
        return text

    def extract_number(self, text: str) -> str:
        """ìˆ«ì ì¶”ì¶œ"""
        if not text:
            return ""
        numbers = re.findall(r'[\d,]+', text)
        return numbers[0] if numbers else ""

    def extract_products_from_html(self, html: str, page_number: int) -> List[ProductData]:
        """HTMLì—ì„œ ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ"""
        self.logger.info(f"í˜ì´ì§€ {page_number} ë°ì´í„° ì¶”ì¶œ ì‹œì‘")

        soup = BeautifulSoup(html, 'html.parser')

        # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
        product_list = soup.find('ul', id='productList')
        if not product_list:
            self.logger.warning("ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # ê°œë³„ ìƒí’ˆ ìš”ì†Œë“¤
        product_elements = product_list.find_all('li', class_='baby-product')
        self.logger.info(f"í˜ì´ì§€ {page_number}ì—ì„œ {len(product_elements)}ê°œ ìƒí’ˆ ë°œê²¬")

        products = []
        for i, element in enumerate(product_elements, 1):
            product_data = self._extract_single_product(element, page_number)
            if product_data:
                products.append(product_data)
                self.logger.debug(f"[{page_number}-{i:2d}] {product_data.product_name[:40]}...")

        self.logger.info(f"í˜ì´ì§€ {page_number}ì—ì„œ {len(products)}ê°œ ìƒí’ˆ ì¶”ì¶œ ì™„ë£Œ")
        return products

    def _extract_single_product(self, element: Tag, page_number: int) -> Optional[ProductData]:
        """ê°œë³„ ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ê¸°ë³¸ ì •ë³´
            product_id = element.get('id', '')
            vendor_item_id = element.get('data-vendor-item-id', '')

            # ìƒí’ˆ ë§í¬
            product_link = element.find('a', class_='baby-product-link')
            if not product_link:
                return None

            item_id = product_link.get('data-item-id', '')
            product_url = urljoin(self.config.base_url, product_link.get('href', ''))

            # ìƒí’ˆëª…
            name_element = element.find('div', class_='name')
            product_name = self.clean_text(name_element.get_text() if name_element else "")

            # ê°€ê²© ì •ë³´
            price_info = self._extract_price_info(element)

            # í‰ì  ë° ë¦¬ë·°
            rating_info = self._extract_rating_info(element)

            # ì´ë¯¸ì§€ URL
            img_element = element.find('img')
            image_url = img_element.get('src', '') if img_element else ""

            # ë°°ì†¡ ì •ë³´
            delivery_element = element.find('span', class_='arrival-info')
            delivery_info = self.clean_text(delivery_element.get_text() if delivery_element else "")

            # ë¡œì¼“ë°°ì†¡ ì—¬ë¶€
            is_rocket = bool(element.find('span', class_='badge rocket'))

            # ì ë¦½ê¸ˆ
            cashback_element = element.find('span', class_='reward-cash-txt')
            cashback_amount = self.clean_text(cashback_element.get_text() if cashback_element else "")

            return ProductData(
                product_id=product_id,
                product_name=product_name,
                price=price_info['current_price'],
                original_price=price_info['original_price'],
                discount_rate=price_info['discount_rate'],
                unit_price=price_info['unit_price'],
                rating=rating_info['rating'],
                review_count=rating_info['review_count'],
                product_url=product_url,
                image_url=image_url,
                delivery_info=delivery_info,
                cashback_amount=cashback_amount,
                is_rocket_delivery=is_rocket,
                vendor_item_id=vendor_item_id,
                item_id=item_id,
                page_number=page_number,
                crawled_at=datetime.now().isoformat()
            )

        except Exception as e:
            self.logger.error(f"ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _extract_price_info(self, element: Tag) -> Dict[str, str]:
        """ê°€ê²© ì •ë³´ ì¶”ì¶œ"""
        price_info = {
            'current_price': '',
            'original_price': '',
            'discount_rate': '',
            'unit_price': ''
        }

        # í˜„ì¬ ê°€ê²©
        price_element = element.find('strong', class_='price-value')
        if price_element:
            price_info['current_price'] = self.extract_number(price_element.get_text())

        # í• ì¸ìœ¨
        discount_element = element.find('span', class_='discount-percentage')
        if discount_element:
            price_info['discount_rate'] = discount_element.get_text().strip()

        # ì›ê°€
        original_price_element = element.find('del', class_='base-price')
        if original_price_element:
            price_info['original_price'] = self.extract_number(original_price_element.get_text())

        # ë‹¨ìœ„ê°€ê²©
        unit_price_element = element.find('span', class_='unit-price')
        if unit_price_element:
            price_info['unit_price'] = self.clean_text(unit_price_element.get_text())

        return price_info

    def _extract_rating_info(self, element: Tag) -> Dict[str, str]:
        """í‰ì  ë° ë¦¬ë·° ì •ë³´ ì¶”ì¶œ"""
        rating_info = {
            'rating': '',
            'review_count': ''
        }

        # í‰ì 
        rating_element = element.find('em', class_='rating')
        if rating_element:
            style = rating_element.get('style', '')
            width_match = re.search(r'width:(\d+)%', style)
            if width_match:
                width_percent = int(width_match.group(1))
                rating_info['rating'] = str(width_percent / 20)

        # ë¦¬ë·° ìˆ˜
        review_count_element = element.find('span', class_='rating-total-count')
        if review_count_element:
            review_text = review_count_element.get_text()
            review_count = re.sub(r'[^\d,]', '', review_text)
            rating_info['review_count'] = review_count

        return rating_info


# =================== ë°ì´í„° ì €ì¥ í´ë˜ìŠ¤ ===================
class DataStorage:
    """ë°ì´í„° ì €ì¥ ê´€ë¦¬"""

    def __init__(self, config: CrawlingConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def save_to_csv(self, products: List[ProductData]) -> str:
        """CSV ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = self.config.csv_filename.format(timestamp=timestamp)
        filepath = self.output_dir / filename

        with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
            headers = [
                'ìƒí’ˆID', 'ìƒí’ˆëª…', 'í˜„ì¬ê°€ê²©', 'ì •ê°€', 'í• ì¸ìœ¨', 'ë‹¨ìœ„ê°€ê²©',
                'í‰ì ', 'ë¦¬ë·°ìˆ˜', 'ìƒí’ˆURL', 'ì´ë¯¸ì§€URL', 'ë°°ì†¡ì •ë³´',
                'ì ë¦½ê¸ˆ', 'ë¡œì¼“ë°°ì†¡ì—¬ë¶€', 'íŒë§¤ììƒí’ˆID', 'ì•„ì´í…œID',
                'í˜ì´ì§€ë²ˆí˜¸', 'ìˆ˜ì§‘ì‹œê°„'
            ]

            writer = csv.writer(csvfile)
            writer.writerow(headers)

            for product in products:
                row = [
                    product.product_id, product.product_name, product.price,
                    product.original_price, product.discount_rate, product.unit_price,
                    product.rating, product.review_count, product.product_url,
                    product.image_url, product.delivery_info, product.cashback_amount,
                    product.is_rocket_delivery, product.vendor_item_id, product.item_id,
                    product.page_number, product.crawled_at
                ]
                writer.writerow(row)

        self.logger.info(f"CSV ì €ì¥ ì™„ë£Œ: {filepath} ({len(products)}ê°œ ìƒí’ˆ)")
        return str(filepath)

    def save_to_json(self, products: List[ProductData]) -> str:
        """JSON ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = self.config.json_filename.format(timestamp=timestamp)
        filepath = self.output_dir / filename

        products_dict = [asdict(product) for product in products]

        with open(filepath, 'w', encoding='utf-8') as jsonfile:
            json.dump(products_dict, jsonfile, ensure_ascii=False, indent=2)

        self.logger.info(f"JSON ì €ì¥ ì™„ë£Œ: {filepath}")
        return str(filepath)


# =================== ë©”ì¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ===================
class Coupangí™ˆí”Œë˜ë‹›Crawler:
    """ì¿ íŒ¡ ê³°ê³° ë¸Œëœë“œ í¬ë¡¤ëŸ¬"""

    def __init__(self, config: CrawlingConfig = None):
        self.config = config or CrawlingConfig()
        self.logger = LoggerManager.setup_logger(self.config)
        self.session_manager = SessionManager(self.config, self.logger)
        self.driver_manager = SeleniumDriverManager(self.config, self.logger)
        self.data_extractor = CoupangDataExtractor(self.config, self.logger)
        self.storage = DataStorage(self.config, self.logger)

        self.all_products = []

    def run_crawling(self) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        self.logger.info(f"ğŸš€ ì¿ íŒ¡ '{self.config.brand_name}' ë¸Œëœë“œ í¬ë¡¤ë§ ì‹œì‘")
        self.logger.info(f"ğŸ“Š ìˆ˜ì§‘ ì˜ˆì • í˜ì´ì§€: {self.config.max_pages}ê°œ")

        start_time = datetime.now()

        try:
            # Selenium ë“œë¼ì´ë²„ ì„¤ì •
            self.driver_manager.setup_driver()

            # í˜ì´ì§€ë³„ í¬ë¡¤ë§
            for page in range(1, self.config.max_pages + 1):
                self.logger.info(f"ğŸ“„ í˜ì´ì§€ {page}/{self.config.max_pages} í¬ë¡¤ë§ ì‹œì‘")

                success = self._crawl_single_page(page)
                if not success:
                    self.logger.warning(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
                    continue

                # í˜ì´ì§€ ê°„ ë”œë ˆì´
                if page < self.config.max_pages:
                    delay = random.uniform(*self.config.delay_range)
                    self.logger.info(f"â³ {delay:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(delay)

            # ë°ì´í„° ì €ì¥
            if self.all_products:
                csv_path = self.storage.save_to_csv(self.all_products)
                json_path = self.storage.save_to_json(self.all_products)

                execution_time = datetime.now() - start_time

                result = {
                    'status': 'success',
                    'total_products': len(self.all_products),
                    'pages_crawled': self.config.max_pages,
                    'csv_file': csv_path,
                    'json_file': json_path,
                    'execution_time': str(execution_time),
                    'products_per_page': self._get_products_per_page_stats()
                }

                self.logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(self.all_products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘")
                self.logger.info(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {execution_time}")

                return result
            else:
                return {
                    'status': 'error',
                    'message': 'ìˆ˜ì§‘ëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.'
                }

        except Exception as e:
            self.logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return {
                'status': 'error',
                'message': str(e)
            }
        finally:
            self.driver_manager.close()

    def _crawl_single_page(self, page: int) -> bool:
        """ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            # í˜ì´ì§€ ì´ë™
            success = self.driver_manager.navigate_to_page(
                self.config.brand_url, page
            )
            if not success:
                return False

            # ì½˜í…ì¸  ë¡œë“œ
            html_content = self.driver_manager.scroll_and_load_content()

            # ë°ì´í„° ì¶”ì¶œ
            products = self.data_extractor.extract_products_from_html(html_content, page)

            # ê²°ê³¼ ì €ì¥
            self.all_products.extend(products)

            self.logger.info(f"âœ… í˜ì´ì§€ {page} ì™„ë£Œ: {len(products)}ê°œ ìƒí’ˆ ì¶”ì¶œ")
            return True

        except Exception as e:
            self.logger.error(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return False

    def _get_products_per_page_stats(self) -> Dict[int, int]:
        """í˜ì´ì§€ë³„ ìƒí’ˆ ìˆ˜ í†µê³„"""
        stats = {}
        for product in self.all_products:
            page = product.page_number
            stats[page] = stats.get(page, 0) + 1
        return stats


# =================== ì‹¤í–‰ í•¨ìˆ˜ ===================
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ›’ ì¿ íŒ¡ ê³°ê³° ë¸Œëœë“œ í¬ë¡¤ë§ ì‹œìŠ¤í…œ")
    print("=" * 50)

    # Selenium ì„¤ì¹˜ í™•ì¸
    if not SELENIUM_AVAILABLE:
        print("âŒ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ“¦ ì„¤ì¹˜ ëª…ë ¹ì–´: pip install selenium")
        print("ğŸ”§ Chrome ë“œë¼ì´ë²„ë„ í•„ìš”í•©ë‹ˆë‹¤: https://chromedriver.chromium.org/")
        return

    # ì„¤ì •
    config = CrawlingConfig(
        max_pages=3,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 3í˜ì´ì§€ë§Œ
        headless=False,  # ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ (ë””ë²„ê¹…ìš©)
        delay_range=(1.0, 1.1),
        log_level="INFO"
    )

    print(f"ğŸ¯ ë¸Œëœë“œ: {config.brand_name}")
    print(f"ğŸ“„ ìˆ˜ì§‘ í˜ì´ì§€: {config.max_pages}ê°œ")
    print(f"ğŸ’¾ ì¶œë ¥ í´ë”: {config.output_dir}")
    print("-" * 50)

    # í¬ë¡¤ë§ ì‹¤í–‰
    crawler = Coupangí™ˆí”Œë˜ë‹›Crawler(config)
    result = crawler.run_crawling()

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    if result['status'] == 'success':
        print("ğŸ‰ í¬ë¡¤ë§ ì„±ê³µ!")
        print(f"ğŸ“Š ì´ ìƒí’ˆ ìˆ˜: {result['total_products']:,}ê°œ")
        print(f"ğŸ“ CSV íŒŒì¼: {result['csv_file']}")
        print(f"ğŸ“ JSON íŒŒì¼: {result['json_file']}")
        print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {result['execution_time']}")

        print("\nğŸ“ˆ í˜ì´ì§€ë³„ ìƒí’ˆ ìˆ˜:")
        for page, count in result['products_per_page'].items():
            print(f"  í˜ì´ì§€ {page}: {count:,}ê°œ")
    else:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {result['message']}")


if __name__ == "__main__":
    main()