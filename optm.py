import asyncio
import aiohttp
import time
import random
import ssl
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field
from collections import defaultdict
import json
import threading
from concurrent.futures import ThreadPoolExecutor
from bs4 import BeautifulSoup as bs
from pathlib import Path
from openpyxl import Workbook
from fake_useragent import UserAgent
import os
import re
import itertools
from urllib.parse import urlencode

# ê¸°ì¡´ í´ë˜ìŠ¤ë“¤ import (ê·¸ëŒ€ë¡œ ìœ ì§€)
from main3 import (
    NonWindowsUserAgent, SaveData, URLManager,
    load_proxy_list_from_file, create_sample_proxy_file,
    is_valid_proxy_format, test_proxy
)


@dataclass
class ProxyStats:
    """í”„ë¡ì‹œ í†µê³„ ì •ë³´"""
    success_count: int = 0
    failure_count: int = 0
    last_used: float = 0
    avg_response_time: float = 0
    total_response_time: float = 0

    @property
    def success_rate(self) -> float:
        total = self.success_count + self.failure_count
        return self.success_count / total if total > 0 else 0.5

    @property
    def performance_score(self) -> float:
        """ì„±ëŠ¥ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)"""
        if self.avg_response_time == 0:
            return self.success_rate

        # ì„±ê³µë¥ ê³¼ ì‘ë‹µì‹œê°„ì„ ì¡°í•©í•œ ì ìˆ˜
        time_score = min(1.0, 3.0 / max(self.avg_response_time, 0.1))
        return (self.success_rate * 0.7) + (time_score * 0.3)


class AsyncProxyManager:
    """ë¹„ë™ê¸° í”„ë¡ì‹œ ê´€ë¦¬ì (í”„ë¡ì‹œë‹¹ 1ê°œ ì—°ê²°)"""

    def __init__(self, proxy_list: List[str], max_concurrent_per_proxy: int = 1):
        self.proxy_list = proxy_list or []
        self.max_concurrent_per_proxy = max_concurrent_per_proxy  # 1ê°œë¡œ ì œí•œ
        self.proxy_stats = {proxy: ProxyStats() for proxy in self.proxy_list}
        self.failed_proxies = set()
        self.lock = asyncio.Lock()
        # ì„¸ë§ˆí¬ì–´ ëŒ€ì‹  ê°„ë‹¨í•œ ì—°ê²° ì¹´ìš´í„° ì‚¬ìš©
        self.active_connections = defaultdict(int)

    async def get_best_proxy(self) -> Optional[str]:
        """ì„±ëŠ¥ ê¸°ë°˜ ìµœì  í”„ë¡ì‹œ ì„ íƒ (ë³´ìˆ˜ì  ê¸°ì¤€)"""
        async with self.lock:
            available_proxies = [
                p for p in self.proxy_list
                if p not in self.failed_proxies and
                   self.active_connections[p] < self.max_concurrent_per_proxy
            ]

            if not available_proxies:
                # 70% ì´ìƒì˜ í”„ë¡ì‹œê°€ ì‹¤íŒ¨í–ˆë‹¤ë©´ ì‹¤íŒ¨ ëª©ë¡ ì´ˆê¸°í™” (ê¸°ì¡´ 80%ì—ì„œ ê°ì†Œ)
                if len(self.failed_proxies) > len(self.proxy_list) * 0.7:
                    print("[WARNING] 70% ì´ìƒì˜ í”„ë¡ì‹œê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì‹¤íŒ¨ ëª©ë¡ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                    self.failed_proxies.clear()
                    available_proxies = [
                        p for p in self.proxy_list
                        if self.active_connections[p] < self.max_concurrent_per_proxy
                    ]

            if not available_proxies:
                return None

            # ì„±ëŠ¥ ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì„ íƒ
            weights = [self.proxy_stats[proxy].performance_score for proxy in available_proxies]
            total_weight = sum(weights)

            if total_weight == 0:
                return random.choice(available_proxies)

            # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ëœë¤ ì„ íƒ
            r = random.random() * total_weight
            cumulative_weight = 0

            for proxy, weight in zip(available_proxies, weights):
                cumulative_weight += weight
                if r <= cumulative_weight:
                    return proxy

            return available_proxies[-1]

    async def acquire_proxy(self, proxy: str) -> bool:
        """í”„ë¡ì‹œ ì‚¬ìš© ì‹œì‘"""
        async with self.lock:
            if self.active_connections[proxy] < self.max_concurrent_per_proxy:
                self.active_connections[proxy] += 1
                self.proxy_stats[proxy].last_used = time.time()
                return True
            return False

    async def release_proxy(self, proxy: str):
        """í”„ë¡ì‹œ ì‚¬ìš© ì¢…ë£Œ"""
        async with self.lock:
            if self.active_connections[proxy] > 0:
                self.active_connections[proxy] -= 1

    async def record_success(self, proxy: str, response_time: float):
        """ì„±ê³µ ê¸°ë¡"""
        async with self.lock:
            stats = self.proxy_stats[proxy]
            stats.success_count += 1
            stats.total_response_time += response_time
            stats.avg_response_time = stats.total_response_time / stats.success_count

    async def record_failure(self, proxy: str):
        """ì‹¤íŒ¨ ê¸°ë¡ (ë” ì—„ê²©í•œ ê¸°ì¤€)"""
        async with self.lock:
            stats = self.proxy_stats[proxy]
            stats.failure_count += 1

            # ì‹¤íŒ¨ìœ¨ì´ ë†’ìœ¼ë©´ ë” ë¹ ë¥´ê²Œ ì œì™¸ (ê¸°ì¤€ ê°•í™”)
            if stats.failure_count > 5 and stats.success_rate < 0.5:  # 5íšŒ ì‹¤íŒ¨ í›„ ì„±ê³µë¥  50% ë¯¸ë§Œ
                self.failed_proxies.add(proxy)
                print(f"[WARNING] í”„ë¡ì‹œ ì¼ì‹œ ì œì™¸: {proxy.split(':')[0]} (ì„±ê³µë¥ : {stats.success_rate:.2f})")
            elif stats.failure_count > 3:  # 3íšŒ ì´ìƒ ì‹¤íŒ¨ì‹œ ê²½ê³ 
                print(f"[WARNING] í”„ë¡ì‹œ ì‹¤íŒ¨ ì¦ê°€: {proxy.split(':')[0]} (ì‹¤íŒ¨: {stats.failure_count}íšŒ)")

    def get_proxy_dict(self, proxy_string: str) -> Optional[Dict[str, str]]:
        """í”„ë¡ì‹œ ë¬¸ìì—´ì„ aiohttpìš© ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        if not proxy_string:
            return None

        parts = proxy_string.split(':')
        if len(parts) == 2:  # ip:port
            ip, port = parts
            return f'http://{ip}:{port}'
        elif len(parts) == 4:  # ip:port:username:password
            ip, port, username, password = parts
            return f'http://{username}:{password}@{ip}:{port}'
        return None


class AsyncCoupangCrawler:
    """ë¹„ë™ê¸° ì¿ íŒ¡ í¬ë¡¤ëŸ¬"""

    def __init__(self, proxy_list: List[str] = None, max_concurrent: int = 80):
        # ê¸°ë³¸ ì„¤ì • (ë†’ì€ ë™ì‹œì„± + ì•ˆì „ì„±)
        self.base_review_url = "https://www.coupang.com/vp/product/reviews"
        self.max_concurrent = min(max_concurrent, 80)  # ìµœëŒ€ 80ê°œ ë™ì‹œ ìš”ì²­
        self.max_pages_per_product = 100  # í˜ì´ì§€ ìˆ˜ëŠ” 100ê°œë¡œ ìœ ì§€
        self.max_retries = 2  # ì¬ì‹œë„ íšŸìˆ˜ëŠ” 2íšŒë¡œ ìœ ì§€

        # ë¹„ë™ê¸° ê´€ë¦¬ìë“¤ (í”„ë¡ì‹œë‹¹ 1ê°œ ì—°ê²°)
        self.proxy_manager = AsyncProxyManager(proxy_list, max_concurrent_per_proxy=1)  # í”„ë¡ì‹œë‹¹ 1ê°œë¡œ ì œí•œ
        self.global_semaphore = asyncio.Semaphore(self.max_concurrent)

        # User-Agent ê´€ë¦¬
        self.ua = NonWindowsUserAgent()

        # URL ê´€ë¦¬
        self.url_manager = URLManager()

        # í†µê³„
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0

        # SSL ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        self.ssl_context = self._create_ssl_context()

    def _create_ssl_context(self):
        """SSL ê²€ì¦ì„ ë¹„í™œì„±í™”í•œ SSL ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            return ssl_context
        except Exception as e:
            print(f"[WARNING] SSL ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return False  # SSL ê²€ì¦ ì™„ì „ ë¹„í™œì„±í™”

    def get_realistic_headers(self) -> Dict[str, str]:
        """ë”ìš± ì‹¤ì œ ë¸Œë¼ìš°ì €ì™€ ìœ ì‚¬í•œ í—¤ë” ìƒì„±"""
        user_agent = self.ua.random

        # ê¸°ë³¸ ë¸Œë¼ìš°ì € í—¤ë” (ì‹¤ì œ Chromeì—ì„œ ë³µì‚¬)
        headers = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Cache-Control": "max-age=0",
            "Connection": "keep-alive",
            "DNT": "1",
            "Pragma": "no-cache",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "same-origin",
            "Sec-Fetch-User": "?1",
            "Upgrade-Insecure-Requests": "1",
            "User-Agent": user_agent,
        }

        # User-Agentì— ë”°ë¼ í”Œë«í¼ ì •ë³´ ì¡°ì •
        if 'iPhone' in user_agent or 'iPad' in user_agent:
            headers.update({
                "sec-ch-ua-platform": '"iOS"',
                "sec-ch-ua-mobile": "?1" if 'iPhone' in user_agent else "?0"
            })
        elif 'Android' in user_agent:
            headers.update({
                "sec-ch-ua-platform": '"Android"',
                "sec-ch-ua-mobile": "?1"
            })
        elif 'Macintosh' in user_agent or 'Mac OS X' in user_agent:
            headers.update({
                "sec-ch-ua-platform": '"macOS"',
                "sec-ch-ua-mobile": "?0"
            })
        else:
            headers.update({
                "sec-ch-ua-platform": '"macOS"',
                "sec-ch-ua-mobile": "?0"
            })

        # ì¿ íŒ¡ íŠ¹í™” í—¤ë” (í•„ìš”ì‹œì—ë§Œ)
        if random.choice([True, False]):
            headers.update({
                "x-coupang-target-market": "KR",
                "x-coupang-accept-language": "ko-KR",
            })

        # ëœë¤ ìš”ì†Œ ì¶”ê°€ (50% í™•ë¥ )
        if random.choice([True, False]):
            headers["Priority"] = "u=0, i"

        return headers

    async def make_request(self, session: aiohttp.ClientSession, url: str,
                           params: Dict, proxy: str) -> Optional[Tuple[str, float]]:
        """ë‹¨ì¼ HTTP ìš”ì²­ ìˆ˜í–‰"""
        start_time = time.time()

        try:
            proxy_url = self.proxy_manager.get_proxy_dict(proxy) if proxy else None

            # ìš”ì²­ í—¤ë”ì— ì¶”ê°€ ì •ë³´
            headers = {
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }

            async with session.get(
                    url,
                    params=params,
                    proxy=proxy_url,
                    ssl=self.ssl_context,  # ì»¤ìŠ¤í…€ SSL ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=30),
                    allow_redirects=True
            ) as response:
                response_time = time.time() - start_time

                if response.status == 200:
                    content = await response.text()
                    if proxy:
                        await self.proxy_manager.record_success(proxy, response_time)
                    return content, response_time
                elif response.status == 403:
                    print(f"[WARNING] HTTP 403 - í”„ë¡ì‹œ ì°¨ë‹¨: {proxy.split(':')[0] if proxy else 'No proxy'}")
                    if proxy:
                        await self.proxy_manager.record_failure(proxy)
                    return None, response_time
                else:
                    print(f"[WARNING] HTTP {response.status}")
                    if proxy:
                        await self.proxy_manager.record_failure(proxy)
                    return None, response_time

        except asyncio.TimeoutError:
            if proxy:
                await self.proxy_manager.record_failure(proxy)
            print(f"[ERROR] íƒ€ì„ì•„ì›ƒ: {proxy.split(':')[0] if proxy else 'No proxy'}")
            return None, time.time() - start_time
        except ssl.SSLError as e:
            if proxy:
                await self.proxy_manager.record_failure(proxy)
            print(f"[ERROR] SSL ì˜¤ë¥˜: {e}")
            return None, time.time() - start_time
        except Exception as e:
            if proxy:
                await self.proxy_manager.record_failure(proxy)
            print(f"[ERROR] ìš”ì²­ ì‹¤íŒ¨: {e}")
            return None, time.time() - start_time

    async def fetch_page_with_retry(self, session: aiohttp.ClientSession,
                                    payload: Dict, max_retries: int = 2) -> Optional[str]:
        """ì¬ì‹œë„ê°€ í¬í•¨ëœ í˜ì´ì§€ ìš”ì²­ (ë³´ìˆ˜ì  ì ‘ê·¼)"""
        async with self.global_semaphore:

            # í”„ë¡ì‹œ ì‹¤íŒ¨ìœ¨ì´ ë†’ìœ¼ë©´ í”„ë¡ì‹œ ì—†ì´ ì‹œë„ (ê¸°ì¤€ ê°•í™”: 80% â†’ 70%)
            proxy_failure_rate = len(self.proxy_manager.failed_proxies) / max(len(self.proxy_manager.proxy_list),
                                                                              1) if self.proxy_manager.proxy_list else 0
            use_proxy = proxy_failure_rate < 0.7  # 70% ì´ìƒ ì‹¤íŒ¨í•˜ë©´ í”„ë¡ì‹œ ì‚¬ìš© ì•ˆí•¨

            if proxy_failure_rate > 0.5:  # 50% ì´ìƒ ì‹¤íŒ¨ì‹œ ê²½ê³ 
                print(f"[WARNING] í”„ë¡ì‹œ ì‹¤íŒ¨ìœ¨ {proxy_failure_rate:.1%} - ì§ì ‘ ì—°ê²° ê°€ëŠ¥ì„± ì¦ê°€")

            for attempt in range(max_retries):
                if use_proxy and self.proxy_manager.proxy_list:
                    # í”„ë¡ì‹œ ì‚¬ìš© ì‹œë„
                    proxy = await self.proxy_manager.get_best_proxy()

                    if proxy and await self.proxy_manager.acquire_proxy(proxy):
                        try:
                            result, response_time = await self.make_request(
                                session, self.base_review_url, payload, proxy
                            )

                            if result:
                                self.successful_requests += 1
                                return result
                            else:
                                self.failed_requests += 1

                        finally:
                            await self.proxy_manager.release_proxy(proxy)
                    else:
                        # ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œê°€ ì—†ìœ¼ë©´ í”„ë¡ì‹œ ì—†ì´ ì‹œë„
                        use_proxy = False

                if not use_proxy:
                    # í”„ë¡ì‹œ ì—†ì´ ì§ì ‘ ìš”ì²­
                    print(f"[DEBUG] í”„ë¡ì‹œ ì—†ì´ ì§ì ‘ ìš”ì²­ ì‹œë„... (ì‹œë„ {attempt + 1}/{max_retries})")
                    result, response_time = await self.make_request(
                        session, self.base_review_url, payload, None
                    )

                    if result:
                        self.successful_requests += 1
                        return result
                    else:
                        self.failed_requests += 1

                # ì¬ì‹œë„ ì „ ëŒ€ê¸° (ë” ê¸´ ë”œë ˆì´)
                if attempt < max_retries - 1:
                    delay = random.uniform(3, 7)  # 3-7ì´ˆ ëŒ€ê¸° (ê¸°ì¡´ 2-5ì´ˆì—ì„œ ì¦ê°€)
                    print(f"[DEBUG] ì¬ì‹œë„ ì „ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                    await asyncio.sleep(delay)

            self.total_requests += 1
            return None

    async def parse_review_page(self, html_content: str, page_num: int,
                                sd: SaveData, product_title: str) -> int:
        """ë¦¬ë·° í˜ì´ì§€ íŒŒì‹± ë° ì €ì¥"""
        try:
            soup = bs(html_content, "html.parser")
            articles = soup.select("article.sdp-review__article__list")

            if not articles:
                return 0

            print(f"[SUCCESS] í˜ì´ì§€ {page_num}ì—ì„œ {len(articles)}ê°œ ë¦¬ë·° ë°œê²¬")

            # ë¦¬ë·° ë°ì´í„° íŒŒì‹±
            reviews_data = []
            for article in articles:
                review_data = self.extract_review_data(article, product_title)
                if review_data:
                    reviews_data.append(review_data)

            # ë°°ì¹˜ë¡œ ì €ì¥ (Thread pool ì‚¬ìš©í•˜ì—¬ I/O ë¸”ë¡œí‚¹ ë°©ì§€)
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor(max_workers=2) as executor:
                for review_data in reviews_data:
                    await loop.run_in_executor(executor, sd.save, review_data)

            return len(reviews_data)

        except Exception as e:
            print(f"[ERROR] í˜ì´ì§€ {page_num} íŒŒì‹± ì‹¤íŒ¨: {e}")
            return 0

    def extract_review_data(self, article, product_title: str) -> Optional[Dict]:
        """ë‹¨ì¼ ë¦¬ë·° ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        try:
            # ê¸°ì¡´ ì¶”ì¶œ ë¡œì§ê³¼ ë™ì¼
            review_date_elem = article.select_one(
                "div.sdp-review__article__list__info__product-info__reg-date"
            )
            review_date = review_date_elem.text.strip() if review_date_elem else "-"

            user_name_elem = article.select_one(
                "span.sdp-review__article__list__info__user__name"
            )
            user_name = user_name_elem.text.strip() if user_name_elem else "-"

            rating_elem = article.select_one(
                "div.sdp-review__article__list__info__product-info__star-orange"
            )
            if rating_elem and rating_elem.get("data-rating"):
                try:
                    rating = int(rating_elem.get("data-rating"))
                except (ValueError, TypeError):
                    rating = 0
            else:
                rating = 0

            prod_name_elem = article.select_one(
                "div.sdp-review__article__list__info__product-info__name"
            )
            prod_name = prod_name_elem.text.strip() if prod_name_elem else "-"

            headline_elem = article.select_one(
                "div.sdp-review__article__list__headline"
            )
            headline = headline_elem.text.strip() if headline_elem else "ë“±ë¡ëœ í—¤ë“œë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤"

            review_content_elem = article.select_one(
                "div.sdp-review__article__list__review__content.js_reviewArticleContent"
            )
            if review_content_elem:
                review_content = re.sub("[\n\t]", "", review_content_elem.text.strip())
            else:
                review_content_elem = article.select_one(
                    "div.sdp-review__article__list__review > div"
                )
                if review_content_elem:
                    review_content = re.sub("[\n\t]", "", review_content_elem.text.strip())
                else:
                    review_content = "ë“±ë¡ëœ ë¦¬ë·°ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"

            helpful_count_elem = article.select_one("span.js_reviewArticleHelpfulCount")
            helpful_count = helpful_count_elem.text.strip() if helpful_count_elem else "0"

            review_images = article.select("div.sdp-review__article__list__attachment__list img")
            image_count = len(review_images)

            return {
                "title": product_title,
                "prod_name": prod_name,
                "review_date": review_date,
                "user_name": user_name,
                "rating": rating,
                "headline": headline,
                "review_content": review_content,
                "helpful_count": helpful_count,
                "image_count": image_count
            }

        except Exception as e:
            print(f"[ERROR] ë¦¬ë·° ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def crawl_product_pages_batch(self, prod_code: str, product_title: str,
                                        sd: SaveData, batch_size: int = 5) -> int:
        """ìƒí’ˆì˜ ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ë°°ì¹˜ë¡œ í¬ë¡¤ë§ (ë³´ìˆ˜ì  ì ‘ê·¼)"""

        # SSL ê²€ì¦ ë¹„í™œì„±í™”ëœ ì»¤ë„¥í„° ì„¤ì •
        connector = aiohttp.TCPConnector(
            limit=100,  # ì „ì²´ ì—°ê²° í’€ í¬ê¸° ì¦ê°€
            limit_per_host=80,  # í˜¸ìŠ¤íŠ¸ë‹¹ ì—°ê²° ìˆ˜ ì¦ê°€ (80ê°œ ë™ì‹œ ìš”ì²­ ì§€ì›)
            ssl=self.ssl_context,
            enable_cleanup_closed=True,
            force_close=True
        )
        timeout = aiohttp.ClientTimeout(total=45)  # íƒ€ì„ì•„ì›ƒ ìœ ì§€

        async with aiohttp.ClientSession(
                headers=self.get_realistic_headers(),
                connector=connector,
                timeout=timeout
        ) as session:

            total_reviews = 0
            current_page = 1
            consecutive_empty_pages = 0
            max_empty_pages = 3  # ë¹ˆ í˜ì´ì§€ í—ˆìš© íšŸìˆ˜ ê°ì†Œ
            failed_proxy_count = 0
            max_failed_proxies = len(self.proxy_manager.proxy_list) * 0.9 if self.proxy_manager.proxy_list else 0

            while (consecutive_empty_pages < max_empty_pages and
                   current_page <= self.max_pages_per_product):

                # í”„ë¡ì‹œ ëŒ€ë¶€ë¶„ì´ ì‹¤íŒ¨í–ˆìœ¼ë©´ ì¤‘ë‹¨
                if self.proxy_manager.proxy_list and failed_proxy_count > max_failed_proxies:
                    print(f"[WARNING] 90% ì´ìƒì˜ í”„ë¡ì‹œê°€ ì°¨ë‹¨ë˜ì–´ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break

                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ í˜ì´ì§€ ìš”ì²­ ìƒì„± (ë” ì‘ì€ ë°°ì¹˜)
                end_page = min(current_page + batch_size - 1, self.max_pages_per_product)
                batch_tasks = []

                for page_num in range(current_page, end_page + 1):
                    payload = {
                        "productId": str(prod_code),
                        "page": str(page_num),
                        "size": "5",
                        "sortBy": "ORDER_SCORE_ASC",
                        "ratings": "",
                        "q": "",
                        "viRoleCode": "2",
                        "ratingSummary": "true",
                    }

                    task = self.fetch_page_with_retry(session, payload, max_retries=2)
                    batch_tasks.append((page_num, task))

                    # ìš”ì²­ ê°„ ì§§ì€ ë”œë ˆì´ (ë´‡ íƒì§€ ë°©ì§€)
                    await asyncio.sleep(random.uniform(0.5, 1.5))

                # ë°°ì¹˜ ì‹¤í–‰
                print(f"[INFO] í˜ì´ì§€ {current_page}-{end_page} ë°°ì¹˜ ìš”ì²­ ì¤‘... (ë³´ìˆ˜ì  ëª¨ë“œ)")
                batch_results = await asyncio.gather(
                    *[task for _, task in batch_tasks],
                    return_exceptions=True
                )

                # ê²°ê³¼ ì²˜ë¦¬
                batch_review_count = 0
                batch_403_count = 0

                for (page_num, _), result in zip(batch_tasks, batch_results):
                    if isinstance(result, Exception):
                        print(f"[ERROR] í˜ì´ì§€ {page_num} ìš”ì²­ ì‹¤íŒ¨: {result}")
                        continue

                    if result:
                        review_count = await self.parse_review_page(
                            result, page_num, sd, product_title
                        )
                        batch_review_count += review_count

                        if review_count == 0:
                            consecutive_empty_pages += 1
                        else:
                            consecutive_empty_pages = 0
                    else:
                        consecutive_empty_pages += 1
                        batch_403_count += 1

                # 403 ì˜¤ë¥˜ê°€ ë§ìœ¼ë©´ ì‹¤íŒ¨í•œ í”„ë¡ì‹œ ì¹´ìš´íŠ¸ ì¦ê°€
                if batch_403_count > len(batch_tasks) * 0.7:
                    failed_proxy_count += batch_403_count

                total_reviews += batch_review_count
                current_page = end_page + 1

                # ë°°ì¹˜ ê°„ ë” ê¸´ ëŒ€ê¸° (ì¸ê°„ì ì¸ íŒ¨í„´)
                if current_page <= self.max_pages_per_product:
                    delay = random.uniform(3, 8)  # 3-8ì´ˆ ëŒ€ê¸°
                    print(f"[DEBUG] ë‹¤ìŒ ë°°ì¹˜ê¹Œì§€ {delay:.1f}ì´ˆ ëŒ€ê¸°... (íƒì§€ ë°©ì§€)")
                    await asyncio.sleep(delay)

                print(
                    f"[BATCH] í˜ì´ì§€ {current_page - batch_size}-{end_page}: {batch_review_count}ê°œ ë¦¬ë·°, 403 ì˜¤ë¥˜: {batch_403_count}ê°œ")

                # ì—°ì†ìœ¼ë¡œ ëª¨ë“  ìš”ì²­ì´ ì‹¤íŒ¨í•˜ë©´ ì¡°ê¸° ì¢…ë£Œ
                if batch_review_count == 0 and batch_403_count == len(batch_tasks):
                    consecutive_empty_pages += 1
                    print(f"[WARNING] ë°°ì¹˜ ì „ì²´ê°€ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì—°ì† ì‹¤íŒ¨: {consecutive_empty_pages}/{max_empty_pages}")

            return total_reviews

    async def crawl_single_product(self, url: str, product_name: str) -> bool:
        """ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)"""
        if '#' in url:
            url = url.split('#')[0]

        # ìƒí’ˆ ì½”ë“œ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§)
        prod_code = url.split("products/")[-1].split("?")[0]
        print(f"[DEBUG] ìƒí’ˆ ì½”ë“œ: {prod_code}")

        # SaveData ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        sd = SaveData()

        product_start_time = time.time()

        try:
            total_reviews = await self.crawl_product_pages_batch(
                prod_code, product_name, sd
            )

            product_end_time = time.time()
            product_elapsed = product_end_time - product_start_time

            print(f"\n[PRODUCT SUMMARY] ìƒí’ˆ '{product_name}' í¬ë¡¤ë§ ì™„ë£Œ")
            print(f"[INFO] ì´ ë¦¬ë·° ìˆ˜: {total_reviews}ê°œ")
            print(f"[INFO] ì†Œìš” ì‹œê°„: {product_elapsed / 60:.1f}ë¶„")
            print(
                f"[INFO] ì„±ê³µë¥ : {self.successful_requests}/{self.total_requests} ({self.successful_requests / max(self.total_requests, 1) * 100:.1f}%)")

            return total_reviews > 0

        except Exception as e:
            print(f"[ERROR] ìƒí’ˆ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return False

    async def start_async(self) -> None:
        """ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹œì‘ (í”„ë¡ì‹œë‹¹ 1ê°œ ì—°ê²° ëª¨ë“œ)"""
        print("=" * 70)
        print("ğŸš€ ì¿ íŒ¡ ë¦¬ë·° í¬ë¡¤ëŸ¬ v2.2 (í”„ë¡ì‹œ ë¶„ì‚° ëª¨ë“œ)")
        print("=" * 70)

        # JSON íŒŒì¼ ë¡œë“œ
        if not self.url_manager.load_urls_from_json():
            print("[ERROR] JSON íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        total_products = len(self.url_manager.products)
        print(f"[INFO] ì´ {total_products}ê°œ ìƒí’ˆì„ íš¨ìœ¨ì ìœ¼ë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
        print(f"[INFO] ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜: {self.max_concurrent}ê°œ")
        print(f"[INFO] ë°°ì¹˜ í¬ê¸°: 5í˜ì´ì§€ (ì•ˆì „ì„± ìœ ì§€)")
        print(f"[INFO] ìƒí’ˆë‹¹ ìµœëŒ€ í˜ì´ì§€: {self.max_pages_per_product}í˜ì´ì§€")
        print(f"[INFO] ìƒí’ˆ ê°„ ëŒ€ê¸°ì‹œê°„: 15-30ì´ˆ")

        if self.proxy_manager.proxy_list:
            print(f"[INFO] ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œ: {len(self.proxy_manager.proxy_list)}ê°œ")
            print(f"[INFO] í”„ë¡ì‹œë‹¹ ë™ì‹œ ì—°ê²°: {self.proxy_manager.max_concurrent_per_proxy}ê°œ (ì°¨ë‹¨ ë°©ì§€)")
            print(f"[STRATEGY] ë” ë§ì€ í”„ë¡ì‹œë¥¼ ë™ì‹œ í™œìš©í•˜ì—¬ ì²˜ë¦¬ëŸ‰ ìµœëŒ€í™”")
            print(f"[WARNING] í”„ë¡ì‹œ 70% ì´ìƒ ì‹¤íŒ¨ì‹œ ìë™ìœ¼ë¡œ ì§ì ‘ ì—°ê²°ë¡œ ì „í™˜ë©ë‹ˆë‹¤.")
        else:
            print(f"[WARNING] í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰ - ë§¤ìš° ëŠë¦° ì†ë„ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.")

        print("=" * 70)

        # ì „ì²´ í†µê³„
        total_success_products = 0
        total_failed_products = 0
        overall_start_time = time.time()

        # ìƒí’ˆë³„ í¬ë¡¤ë§ ì‹¤í–‰
        for i, product in enumerate(self.url_manager.products, 1):
            print(f"\n{'=' * 20} ìƒí’ˆ {i}/{total_products} {'=' * 20}")
            print(f"[INFO] í˜„ì¬ ìƒí’ˆ: {product['name']}")
            print(f"[INFO] ìƒí’ˆ URL: {product['url']}")

            try:
                success = await self.crawl_single_product(product['url'], product['name'])
                if success:
                    total_success_products += 1
                    print(f"âœ… ìƒí’ˆ {i} í¬ë¡¤ë§ ì„±ê³µ")
                else:
                    total_failed_products += 1
                    print(f"âŒ ìƒí’ˆ {i} í¬ë¡¤ë§ ì‹¤íŒ¨")

            except KeyboardInterrupt:
                print(f"\n[INFO] ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"[ERROR] ìƒí’ˆ í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                total_failed_products += 1
                continue

            # ìƒí’ˆ ê°„ ëŒ€ê¸° ì‹œê°„ (ë” ê¸´ ë”œë ˆì´)
            if i < total_products:
                delay = random.uniform(15, 30)  # 15-30ì´ˆ ëŒ€ê¸° (ë§¤ìš° ë³´ìˆ˜ì )
                print(f"[INFO] ë‹¤ìŒ ìƒí’ˆê¹Œì§€ {delay:.1f}ì´ˆ ëŒ€ê¸°... (ë´‡ íƒì§€ ë°©ì§€)")
                await asyncio.sleep(delay)

        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        overall_end_time = time.time()
        total_elapsed = overall_end_time - overall_start_time

        print("\n" + "=" * 70)
        print("ğŸ“Š ì „ì²´ í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("=" * 70)
        print(f"ì´ ìƒí’ˆ ìˆ˜: {total_products}ê°œ")
        print(f"ì„±ê³µí•œ ìƒí’ˆ: {total_success_products}ê°œ")
        print(f"ì‹¤íŒ¨í•œ ìƒí’ˆ: {total_failed_products}ê°œ")
        print(f"ì„±ê³µë¥ : {(total_success_products / total_products * 100):.1f}%")
        print(f"ì´ ì†Œìš” ì‹œê°„: {total_elapsed / 60:.1f}ë¶„")
        print(f"ì´ ìš”ì²­ ìˆ˜: {self.total_requests}ê°œ")
        print(f"ìš”ì²­ ì„±ê³µë¥ : {(self.successful_requests / max(self.total_requests, 1) * 100):.1f}%")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼ë“¤ì€ 'Coupang-reviews' í´ë”ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
        print("=" * 70)


def start_optimized_crawler():
    """ìµœì í™”ëœ í¬ë¡¤ëŸ¬ ì‹¤í–‰ í•¨ìˆ˜"""

    # í”„ë¡ì‹œ ëª©ë¡ ë¡œë“œ
    proxy_file_path = "proxy_list.txt"
    print("=" * 70)
    print("ğŸ”— í”„ë¡ì‹œ ì„¤ì • (ë³´ìˆ˜ì  ëª¨ë“œ)")
    print("=" * 70)

    proxy_list = load_proxy_list_from_file(proxy_file_path)

    if not proxy_list:
        print(f"\n[WARNING] {proxy_file_path} íŒŒì¼ì— ìœ íš¨í•œ í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
        run_without_proxy = input("í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").lower().strip()
        if run_without_proxy == 'n':
            print("[INFO] í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        proxy_list = None
    else:
        print(f"\n[INFO] {len(proxy_list)}ê°œì˜ í”„ë¡ì‹œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"[NOTICE] ìµœê·¼ ì¿ íŒ¡ì—ì„œ í”„ë¡ì‹œ ì°¨ë‹¨ì´ ê°•í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"[NOTICE] 403 ì˜¤ë¥˜ê°€ ë§ì´ ë°œìƒí•˜ë©´ í”„ë¡ì‹œ ì—†ì´ ìë™ ì „í™˜ë©ë‹ˆë‹¤.")

        use_proxy = input("í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").lower().strip()
        if use_proxy == 'n':
            proxy_list = None

    # ë™ì‹œì„± ì„¤ì • (í”„ë¡ì‹œë‹¹ 1ê°œ ì—°ê²°)
    if proxy_list:
        max_concurrent = min(80, len(proxy_list))  # í”„ë¡ì‹œ ìˆ˜ë§Œí¼, ìµœëŒ€ 80ê°œ
        print(f"[INFO] í”„ë¡ì‹œ í™œìš© ëª¨ë“œ - ìµœëŒ€ ë™ì‹œ ìš”ì²­: {max_concurrent}ê°œ")
        print(f"[INFO] í”„ë¡ì‹œë‹¹ ì—°ê²° ì œí•œ: 1ê°œ (ì°¨ë‹¨ ë°©ì§€)")
        print(f"[INFO] ë°°ì¹˜ í¬ê¸°: 5í˜ì´ì§€ (ì•ˆì „ì„± ìœ ì§€)")
        print(f"[INFO] ì´ {len(proxy_list)}ê°œ í”„ë¡ì‹œë¥¼ ìˆœí™˜ í™œìš©")
        print(f"[INFO] ìš”ì²­ ê°„ ë”œë ˆì´: 3-8ì´ˆ (ë´‡ íƒì§€ ë°©ì§€)")
        print(f"[INFO] SSL ê²€ì¦ ë¹„í™œì„±í™”ë¡œ í”„ë¡ì‹œ í˜¸í™˜ì„± í™•ë³´")
    else:
        max_concurrent = 3  # í”„ë¡ì‹œ ì—†ì´ëŠ” ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
        print(f"[INFO] í”„ë¡ì‹œ ì—†ì´ ìµœëŒ€ ë™ì‹œ ìš”ì²­: {max_concurrent}ê°œ")
        print(f"[WARNING] í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰í•˜ë©´ IP ì°¨ë‹¨ ìœ„í—˜ì´ ë†’ìŠµë‹ˆë‹¤.")
        print(f"[WARNING] ë§¤ìš° ëŠë¦° ì†ë„ë¡œ í¬ë¡¤ë§ë©ë‹ˆë‹¤. (ì•ˆì „ì„± ìš°ì„ )")

    # í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰
    crawler = AsyncCoupangCrawler(proxy_list=proxy_list, max_concurrent=max_concurrent)

    try:
        # ë¹„ë™ê¸° ì‹¤í–‰
        asyncio.run(crawler.start_async())
        print("\nâœ… ëª¨ë“  ìƒí’ˆ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except KeyboardInterrupt:
        print("\n[INFO] ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n[ERROR] í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("=" * 70)
    print("ğŸš€ ì¿ íŒ¡ ë¦¬ë·° í¬ë¡¤ëŸ¬ v2.2 (í”„ë¡ì‹œ ë¶„ì‚° ëª¨ë“œ)")
    print("=" * 70)
    print("âš¡ í”„ë¡ì‹œë‹¹ 1ê°œ ì—°ê²°ë¡œ ìµœëŒ€ 80ê°œ ë™ì‹œ ìš”ì²­")
    print("ğŸ” SSL ê²€ì¦ ë¹„í™œì„±í™”ë¡œ í”„ë¡ì‹œ í˜¸í™˜ì„± í™•ë³´")
    print("ğŸ¯ ë§ì€ í”„ë¡ì‹œë¥¼ ë™ì‹œ í™œìš©í•˜ì—¬ ì²˜ë¦¬ëŸ‰ ìµœëŒ€í™”")
    print("ğŸ›¡ï¸ ì—°ê²° ì œí•œìœ¼ë¡œ ê°œë³„ í”„ë¡ì‹œ ì°¨ë‹¨ ìœ„í—˜ ìµœì†Œí™”")
    print("ğŸ“Š ë°°ì¹˜ í¬ê¸° 5ê°œë¡œ ì•ˆì •ì„±ê³¼ íš¨ìœ¨ì„± ê· í˜•")
    print("=" * 70)
    start_optimized_crawler()