from bs4 import BeautifulSoup as bs
from pathlib import Path
from openpyxl import Workbook
from fake_useragent import UserAgent
from requests.exceptions import RequestException, Timeout, ConnectionError, ProxyError
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
import time
import os
import re
import requests as rq
import math
import sys
import random
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed


class ProxyManager:
    def __init__(self):
        self.working_proxies = []
        self.failed_proxies = set()
        self.current_index = 0
        self.proxy_lock = threading.Lock()

        # ë¬´ë£Œ í”„ë¡ì‹œ ì†ŒìŠ¤ URLë“¤
        self.proxy_sources = [
            "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt",
            "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt",
            "https://raw.githubusercontent.com/proxy4parsing/proxy-list/main/http.txt"
        ]

        # ìœ ë£Œ í”„ë¡ì‹œ ì„¤ì • (ì˜ˆì‹œ)
        self.premium_proxies = [
            # ì˜ˆì‹œ: {"http": "http://user:pass@premium-proxy1.com:8080", "https": "http://user:pass@premium-proxy1.com:8080"},
            # ì˜ˆì‹œ: {"http": "http://user:pass@premium-proxy2.com:8080", "https": "http://user:pass@premium-proxy2.com:8080"},
        ]

    def fetch_free_proxies(self) -> list:
        """ë¬´ë£Œ í”„ë¡ì‹œ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘"""
        all_proxies = []

        for source_url in self.proxy_sources:
            try:
                print(f"[INFO] í”„ë¡ì‹œ ìˆ˜ì§‘ ì¤‘: {source_url}")
                resp = rq.get(source_url, timeout=10)
                if resp.status_code == 200:
                    proxies = resp.text.strip().split('\n')
                    for proxy in proxies:
                        proxy = proxy.strip()
                        if proxy and ':' in proxy:
                            # IP:PORT í˜•ì‹ ê²€ì¦
                            try:
                                ip, port = proxy.split(':')
                                if self.is_valid_ip(ip) and port.isdigit():
                                    proxy_dict = {
                                        "http": f"http://{proxy}",
                                        "https": f"http://{proxy}"
                                    }
                                    all_proxies.append(proxy_dict)
                            except:
                                continue

                print(f"[SUCCESS] {len(all_proxies)}ê°œ í”„ë¡ì‹œ ìˆ˜ì§‘ ì™„ë£Œ")

            except Exception as e:
                print(f"[WARNING] í”„ë¡ì‹œ ìˆ˜ì§‘ ì‹¤íŒ¨ {source_url}: {e}")

        return all_proxies

    def is_valid_ip(self, ip: str) -> bool:
        """IP ì£¼ì†Œ í˜•ì‹ ê²€ì¦"""
        try:
            parts = ip.split('.')
            return len(parts) == 4 and all(0 <= int(part) <= 255 for part in parts)
        except:
            return False

    def test_proxy(self, proxy_dict: dict, timeout: int = 5) -> bool:
        """í”„ë¡ì‹œ ë™ì‘ í…ŒìŠ¤íŠ¸"""
        try:
            test_url = "https://httpbin.org/ip"
            resp = rq.get(
                test_url,
                proxies=proxy_dict,
                timeout=timeout,
                headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
            )
            if resp.status_code == 200:
                # ì‘ë‹µì—ì„œ IP í™•ì¸
                result = resp.json()
                return 'origin' in result
        except:
            pass
        return False

    def validate_proxies_parallel(self, proxy_list: list, max_workers: int = 50) -> list:
        """ë³‘ë ¬ë¡œ í”„ë¡ì‹œ ê²€ì¦"""
        working_proxies = []

        print(f"[INFO] {len(proxy_list)}ê°œ í”„ë¡ì‹œ ê²€ì¦ ì¤‘...")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # í”„ë¡ì‹œ í…ŒìŠ¤íŠ¸ ì‘ì—… ì œì¶œ
            future_to_proxy = {
                executor.submit(self.test_proxy, proxy): proxy
                for proxy in proxy_list
            }

            # ê²°ê³¼ ì²˜ë¦¬
            completed = 0
            for future in as_completed(future_to_proxy):
                proxy = future_to_proxy[future]
                completed += 1

                try:
                    if future.result():
                        working_proxies.append(proxy)
                        print(f"[SUCCESS] ìœ íš¨í•œ í”„ë¡ì‹œ ë°œê²¬: {proxy['http']} ({len(working_proxies)}/ì™„ë£Œ: {completed})")
                except:
                    pass

                # ì§„í–‰ìƒí™© ì¶œë ¥ (ë§¤ 100ê°œë§ˆë‹¤)
                if completed % 100 == 0:
                    print(f"[INFO] ê²€ì¦ ì§„í–‰: {completed}/{len(proxy_list)} (ìœ íš¨: {len(working_proxies)})")

        print(f"[INFO] í”„ë¡ì‹œ ê²€ì¦ ì™„ë£Œ: {len(working_proxies)}ê°œ ìœ íš¨ í”„ë¡ì‹œ ë°œê²¬")
        return working_proxies

    def setup_proxies(self):
        """í”„ë¡ì‹œ ì„¤ì • ë° ê²€ì¦"""
        print("[INFO] í”„ë¡ì‹œ ì„¤ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        # ìœ ë£Œ í”„ë¡ì‹œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if self.premium_proxies:
            print("[INFO] ìœ ë£Œ í”„ë¡ì‹œ ì‚¬ìš©")
            self.working_proxies = self.premium_proxies.copy()
            return

        # ë¬´ë£Œ í”„ë¡ì‹œ ìˆ˜ì§‘ ë° ê²€ì¦
        print("[INFO] ë¬´ë£Œ í”„ë¡ì‹œë¥¼ ìˆ˜ì§‘í•˜ê³  ê²€ì¦í•©ë‹ˆë‹¤ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")

        free_proxies = self.fetch_free_proxies()
        if not free_proxies:
            print("[WARNING] ìˆ˜ì§‘ëœ í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ë¬´ì‘ìœ„ë¡œ ì„ì–´ì„œ ë‹¤ì–‘ì„± í™•ë³´
        random.shuffle(free_proxies)

        # ì²˜ìŒ 500ê°œë§Œ í…ŒìŠ¤íŠ¸ (ì‹œê°„ ì ˆì•½)
        test_proxies = free_proxies[:500] if len(free_proxies) > 500 else free_proxies

        # ë³‘ë ¬ ê²€ì¦
        self.working_proxies = self.validate_proxies_parallel(test_proxies)

        if not self.working_proxies:
            print("[ERROR] ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"[SUCCESS] {len(self.working_proxies)}ê°œì˜ í”„ë¡ì‹œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def get_next_proxy(self) -> dict | None:
        """ë‹¤ìŒ í”„ë¡ì‹œ ë°˜í™˜ (ìˆœí™˜)"""
        with self.proxy_lock:
            if not self.working_proxies:
                return None

            proxy = self.working_proxies[self.current_index]
            self.current_index = (self.current_index + 1) % len(self.working_proxies)
            return proxy

    def mark_proxy_failed(self, proxy_dict: dict):
        """ì‹¤íŒ¨í•œ í”„ë¡ì‹œ ì œê±°"""
        with self.proxy_lock:
            proxy_str = proxy_dict.get('http', '')
            if proxy_str not in self.failed_proxies:
                self.failed_proxies.add(proxy_str)
                # working_proxiesì—ì„œ ì œê±°
                self.working_proxies = [p for p in self.working_proxies if p.get('http') != proxy_str]
                print(f"[WARNING] í”„ë¡ì‹œ ì œê±°: {proxy_str} (ë‚¨ì€ í”„ë¡ì‹œ: {len(self.working_proxies)}ê°œ)")

    def get_proxy_count(self) -> int:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œ ê°œìˆ˜"""
        with self.proxy_lock:
            return len(self.working_proxies)


class ChromeDriver:
    def __init__(self) -> None:
        self.set_options()
        self.set_driver()

    def set_options(self) -> None:
        self.options = Options()
        self.options.add_argument("--headless")
        self.options.add_argument("lang=ko_KR")
        self.options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
        )
        self.options.add_argument("--log-level=3")
        self.options.add_argument("--no-sandbox")
        self.options.add_argument("--disable-dev-shm-usage")
        self.options.add_experimental_option("detach", True)
        self.options.add_experimental_option("excludeSwitches", ["enable-logging"])

    def set_driver(self) -> None:
        self.driver = webdriver.Chrome(options=self.options)


class Coupang:
    @staticmethod
    def get_product_code(url: str) -> str:
        prod_code: str = url.split("products/")[-1].split("?")[0]
        return prod_code

    @staticmethod
    def get_soup_object(resp: rq.Response) -> bs:
        return bs(resp.text, "html.parser")

    def __del__(self) -> None:
        if hasattr(self, 'ch') and self.ch.driver:
            self.ch.driver.quit()
        if hasattr(self, 'session') and self.session:
            self.session.close()

    def __init__(self) -> None:
        self.base_review_url: str = "https://www.coupang.com/vp/product/reviews"
        self.sd = SaveData()
        self.retries = 8
        self.delay_min = 1.5
        self.delay_max = 4.0
        self.page_delay_min = 2.0
        self.page_delay_max = 5.0
        self.timeout_connect = 10
        self.timeout_read = 20

        # í”„ë¡ì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.proxy_manager = ProxyManager()

        self.base_headers = {
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "cache-control": "max-age=0",
            "sec-ch-ua": '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "document",
            "sec-fetch-mode": "navigate",
            "sec-fetch-site": "same-origin",
            "sec-fetch-user": "?1",
            "upgrade-insecure-requests": "1",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
        }

        self.ajax_headers = {
            "accept": "*/*",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "sec-ch-ua": '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-origin",
            "x-requested-with": "XMLHttpRequest",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
        }

        self.ch = ChromeDriver()
        self.page_title = None
        self.session = None
        self.main_url = None
        self.current_proxy = None

        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15"
        ]
        self.current_ua_index = 0

        # í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        self.use_proxy = self.ask_use_proxy()

        if self.use_proxy:
            self.setup_proxy_session()
        else:
            self.setup_session()

    def ask_use_proxy(self) -> bool:
        """í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€ ë¬¼ì–´ë³´ê¸°"""
        while True:
            choice = input("\ní”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
            if choice in ['y', 'yes']:
                return True
            elif choice in ['n', 'no']:
                return False
            else:
                print("y ë˜ëŠ” nì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    def setup_proxy_session(self) -> None:
        """í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ëŠ” ì„¸ì…˜ ì„¤ì •"""
        print("[INFO] í”„ë¡ì‹œ ì„¸ì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤...")

        # í”„ë¡ì‹œ ì„¤ì •
        self.proxy_manager.setup_proxies()

        if self.proxy_manager.get_proxy_count() == 0:
            print("[ERROR] ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ì„¸ì…˜ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            self.use_proxy = False
            self.setup_session()
            return

        self.session = rq.Session()
        self.session.headers.update(self.base_headers)

        # ì²« ë²ˆì§¸ í”„ë¡ì‹œ ì„¤ì •
        self.rotate_proxy()

        print("[INFO] í”„ë¡ì‹œ ì„¸ì…˜ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def setup_session(self) -> None:
        """ì¼ë°˜ ì„¸ì…˜ ì„¤ì •"""
        self.session = rq.Session()
        self.session.headers.update(self.base_headers)
        print("[INFO] ì¼ë°˜ ì„¸ì…˜ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def rotate_proxy(self) -> bool:
        """í”„ë¡ì‹œ ë¡œí…Œì´ì…˜"""
        if not self.use_proxy:
            return True

        new_proxy = self.proxy_manager.get_next_proxy()
        if new_proxy:
            self.current_proxy = new_proxy
            self.session.proxies.update(new_proxy)
            print(f"[INFO] í”„ë¡ì‹œ ë³€ê²½: {new_proxy['http']}")
            return True
        else:
            print("[ERROR] ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

    def get_next_user_agent(self) -> str:
        """ìˆœì°¨ì ìœ¼ë¡œ User-Agent ë³€ê²½"""
        ua = self.user_agents[self.current_ua_index]
        self.current_ua_index = (self.current_ua_index + 1) % len(self.user_agents)
        return ua

    def warm_up_session(self, product_url: str) -> bool:
        """ì„¸ì…˜ ì¤€ë¹„"""
        try:
            print("[INFO] ì„¸ì…˜ ì¤€ë¹„ ì¤‘ - ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸...")

            main_resp = self.session.get(
                "https://www.coupang.com",
                timeout=(self.timeout_connect, self.timeout_read)
            )

            if main_resp.status_code != 200:
                print(f"[WARNING] ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ ì‹¤íŒ¨: {main_resp.status_code}")
                # í”„ë¡ì‹œ ë¬¸ì œì¼ ìˆ˜ ìˆìŒ
                if self.use_proxy and main_resp.status_code in [403, 407, 503]:
                    print("[INFO] í”„ë¡ì‹œ ë³€ê²½ í›„ ì¬ì‹œë„...")
                    if self.current_proxy:
                        self.proxy_manager.mark_proxy_failed(self.current_proxy)
                    if not self.rotate_proxy():
                        return False
                    return self.warm_up_session(product_url)  # ì¬ê·€ í˜¸ì¶œ
            else:
                print("[SUCCESS] ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ ì™„ë£Œ")

            time.sleep(random.uniform(1.0, 2.0))

            print("[INFO] ìƒí’ˆ í˜ì´ì§€ ë°©ë¬¸ ì¤‘...")
            product_resp = self.session.get(
                product_url,
                timeout=(self.timeout_connect, self.timeout_read)
            )

            if product_resp.status_code != 200:
                print(f"[WARNING] ìƒí’ˆ í˜ì´ì§€ ë°©ë¬¸ ì‹¤íŒ¨: {product_resp.status_code}")
                if self.use_proxy and product_resp.status_code in [403, 407, 503]:
                    print("[INFO] í”„ë¡ì‹œ ë³€ê²½ í›„ ì¬ì‹œë„...")
                    if self.current_proxy:
                        self.proxy_manager.mark_proxy_failed(self.current_proxy)
                    if not self.rotate_proxy():
                        return False
                    return self.warm_up_session(product_url)
                return False
            else:
                print("[SUCCESS] ìƒí’ˆ í˜ì´ì§€ ë°©ë¬¸ ì™„ë£Œ")
                self.ajax_headers["referer"] = product_url
                return True

        except (ProxyError, ConnectionError) as e:
            print(f"[ERROR] í”„ë¡ì‹œ/ì—°ê²° ì˜¤ë¥˜: {e}")
            if self.use_proxy and self.current_proxy:
                self.proxy_manager.mark_proxy_failed(self.current_proxy)
                if self.rotate_proxy():
                    return self.warm_up_session(product_url)
            return False
        except Exception as e:
            print(f"[ERROR] ì„¸ì…˜ ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def get_product_title(self, prod_code: str) -> str:
        """ìƒí’ˆëª… ì¶”ì¶œ (Selenium ì‚¬ìš©)"""
        url = f"https://www.coupang.com/vp/products/{prod_code}"
        self.main_url = url
        print(f"[DEBUG] ìƒí’ˆ í˜ì´ì§€ ì ‘ì† ì¤‘: {url}")

        try:
            self.ch.driver.get(url=url)
            print("[DEBUG] í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
            WebDriverWait(self.ch.driver, 30).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            loading_delay = random.uniform(3.0, 6.0)
            print(f"[DEBUG] {loading_delay:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(loading_delay)

            page_source: str = self.ch.driver.page_source
            soup = bs(page_source, "html.parser")

            title_selectors = [
                "h1.prod-buy-header__title",
                ".prod-buy-header__title",
                "h1[class*='title']",
                ".product-title",
                "h1"
            ]

            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.text.strip():
                    title = title_elem.text.strip()
                    print(f"[DEBUG] ìƒí’ˆëª… ë°œê²¬: {title}")
                    return title

            print("[WARNING] ìƒí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "ìƒí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨"

        except Exception as e:
            print(f"[ERROR] get_product_title ì—ëŸ¬: {e}")
            return "ìƒí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨"

    def start(self) -> None:
        self.sd.create_directory()
        URL: str = self.input_review_url()

        if '#' in URL:
            URL = URL.split('#')[0]
            print(f"[DEBUG] URL fragment ì œê±°: {URL}")

        prod_code: str = self.get_product_code(url=URL)
        print(f"[DEBUG] ìƒí’ˆ ì½”ë“œ: {prod_code}")

        try:
            self.title = self.get_product_title(prod_code=prod_code)
            print(f"[INFO] ìƒí’ˆëª…: {self.title}")
        except Exception as e:
            print(f"[ERROR] ìƒí’ˆëª…ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            self.title = "ìƒí’ˆëª… ë¯¸í™•ì¸"

        if not self.warm_up_session(self.main_url):
            print("[ERROR] ì„¸ì…˜ ì¤€ë¹„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            if self.use_proxy:
                print("[INFO] í”„ë¡ì‹œ ì—†ì´ ì¬ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ", end="")
                choice = input().lower().strip()
                if choice in ['y', 'yes']:
                    self.use_proxy = False
                    self.setup_session()
                    if not self.warm_up_session(self.main_url):
                        print("[ERROR] ì¼ë°˜ ì„¸ì…˜ìœ¼ë¡œë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        return
                else:
                    return

        print(f"[INFO] ëª¨ë“  ë¦¬ë·° í˜ì´ì§€ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")

        success_count = 0
        current_page = 1
        consecutive_empty_pages = 0
        max_empty_pages = 3
        consecutive_403_count = 0
        max_403_count = 5

        while consecutive_empty_pages < max_empty_pages and consecutive_403_count < max_403_count:
            payload = {
                "productId": prod_code,
                "page": current_page,
                "size": 5,
                "sortBy": "ORDER_SCORE_ASC",
                "ratings": "",
                "q": "",
                "viRoleCode": 2,
                "ratingSummary": True,
            }

            result = self.fetch(payload=payload)

            if result == "403_error":
                consecutive_403_count += 1
                print(f"[WARNING] ì—°ì† 403 ì˜¤ë¥˜ ë°œìƒ ({consecutive_403_count}/{max_403_count})")

                if self.use_proxy:
                    print("[INFO] í”„ë¡ì‹œ ë³€ê²½ ì‹œë„...")
                    if self.current_proxy:
                        self.proxy_manager.mark_proxy_failed(self.current_proxy)
                    if self.rotate_proxy():
                        consecutive_403_count = 0  # í”„ë¡ì‹œ ë³€ê²½ ì„±ê³µ ì‹œ ë¦¬ì…‹
                        print("[INFO] í”„ë¡ì‹œ ë³€ê²½ ì™„ë£Œ. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                    else:
                        print("[ERROR] ë” ì´ìƒ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
                        break
                else:
                    long_delay = random.uniform(10.0, 20.0)
                    print(f"[INFO] 403 ì˜¤ë¥˜ ë³µêµ¬ë¥¼ ìœ„í•´ {long_delay:.1f}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(long_delay)

            elif result == "proxy_error":
                if self.use_proxy:
                    print("[INFO] í”„ë¡ì‹œ ì˜¤ë¥˜ë¡œ ì¸í•œ í”„ë¡ì‹œ ë³€ê²½...")
                    if self.current_proxy:
                        self.proxy_manager.mark_proxy_failed(self.current_proxy)
                    if not self.rotate_proxy():
                        print("[ERROR] ë” ì´ìƒ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
                        break
                else:
                    print("[ERROR] ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                    break

            elif result:
                success_count += 1
                consecutive_empty_pages = 0
                consecutive_403_count = 0
            else:
                consecutive_empty_pages += 1
                print(f"[WARNING] í˜ì´ì§€ {current_page}ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({consecutive_empty_pages}/{max_empty_pages})")

            current_page += 1

            if result and result not in ["403_error", "proxy_error"]:
                short_delay = random.uniform(1.0, 3.0)
                time.sleep(short_delay)

            if current_page > 1000:
                print("[INFO] ìµœëŒ€ í˜ì´ì§€ ìˆ˜(1000)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                break

        print(f"[INFO] ì´ {success_count}ê°œ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ (ì´ {current_page - 1}í˜ì´ì§€ ì‹œë„)")

        if self.use_proxy:
            print(f"[INFO] ë‚¨ì€ í”„ë¡ì‹œ: {self.proxy_manager.get_proxy_count()}ê°œ")

    def fetch(self, payload: dict) -> bool | str:
        now_page: int = payload["page"]
        print(f"\n[INFO] Start crawling page {now_page} ...")
        attempt: int = 0

        while attempt < self.retries:
            try:
                # ì£¼ê¸°ì ìœ¼ë¡œ í”„ë¡ì‹œ ë° User-Agent ë³€ê²½
                if self.use_proxy and now_page % 5 == 1 and now_page > 1:
                    print("[INFO] ì •ê¸° í”„ë¡ì‹œ ë³€ê²½...")
                    self.rotate_proxy()

                if now_page % 3 == 1:
                    current_ua = self.get_next_user_agent()
                    self.ajax_headers["user-agent"] = current_ua

                resp = self.session.get(
                    url=self.base_review_url,
                    params=payload,
                    headers=self.ajax_headers,
                    timeout=(self.timeout_connect, self.timeout_read),
                )

                if resp.status_code == 403:
                    print(f"[ERROR] HTTP 403 ì‘ë‹µ - ë´‡ ì°¨ë‹¨ ê°ì§€")
                    return "403_error"
                elif resp.status_code in [407, 503, 504]:  # í”„ë¡ì‹œ ê´€ë ¨ ì˜¤ë¥˜
                    print(f"[ERROR] HTTP {resp.status_code} ì‘ë‹µ - í”„ë¡ì‹œ ì˜¤ë¥˜")
                    return "proxy_error"
                elif resp.status_code != 200:
                    print(f"[ERROR] HTTP {resp.status_code} ì‘ë‹µ")
                    attempt += 1
                    time.sleep(random.uniform(2.0, 4.0))
                    continue

                html = resp.text
                soup = bs(html, "html.parser")

                if self.page_title is None:
                    first_review = soup.select_one("article.sdp-review__article__list")
                    if first_review:
                        title_elem = first_review.select_one("div.sdp-review__article__list__info__product-info__name")
                        self.page_title = title_elem.text.strip() if title_elem else self.title
                    else:
                        self.page_title = self.title

                articles = soup.select("article.sdp-review__article__list")
                article_length = len(articles)

                if article_length == 0:
                    print(f"[WARNING] í˜ì´ì§€ {now_page}ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False

                print(f"[SUCCESS] í˜ì´ì§€ {now_page}ì—ì„œ {article_length}ê°œ ë¦¬ë·° ë°œê²¬")

                for idx in range(article_length):
                    dict_data: dict[str, str | int] = dict()

                    # ë¦¬ë·° ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ê³¼ ë™ì¼)
                    review_date_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__reg-date"
                    )
                    review_date = review_date_elem.text.strip() if review_date_elem else "-"

                    user_name_elem = articles[idx].select_one(
                        "span.sdp-review__article__list__info__user__name"
                    )
                    user_name = user_name_elem.text.strip() if user_name_elem else "-"

                    rating_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__star-orange"
                    )
                    if rating_elem and rating_elem.get("data-rating"):
                        try:
                            rating = int(rating_elem.get("data-rating"))
                        except (ValueError, TypeError):
                            rating = 0
                    else:
                        rating = 0

                    prod_name_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__name"
                    )
                    prod_name = prod_name_elem.text.strip() if prod_name_elem else "-"

                    headline_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__headline"
                    )
                    headline = headline_elem.text.strip() if headline_elem else "ë“±ë¡ëœ í—¤ë“œë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤"

                    review_content_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__review__content.js_reviewArticleContent"
                    )
                    if review_content_elem:
                        review_content = re.sub("[\n\t]", "", review_content_elem.text.strip())
                    else:
                        review_content_elem = articles[idx].select_one(
                            "div.sdp-review__article__list__review > div"
                        )
                        if review_content_elem:
                            review_content = re.sub("[\n\t]", "", review_content_elem.text.strip())
                        else:
                            review_content = "ë“±ë¡ëœ ë¦¬ë·°ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"

                    answer_elem = articles[idx].select_one(
                        "span.sdp-review__article__list__survey__row__answer"
                    )
                    answer = answer_elem.text.strip() if answer_elem else "ë§› í‰ê°€ ì—†ìŒ"

                    helpful_count_elem = articles[idx].select_one("span.js_reviewArticleHelpfulCount")
                    helpful_count = helpful_count_elem.text.strip() if helpful_count_elem else "0"

                    seller_name_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__seller_name"
                    )
                    if seller_name_elem:
                        seller_name = seller_name_elem.text.replace("íŒë§¤ì: ", "").strip()
                    else:
                        seller_name = "-"

                    review_images = articles[idx].select("div.sdp-review__article__list__attachment__list img")
                    image_count = len(review_images)

                    dict_data["title"] = self.page_title
                    dict_data["prod_name"] = prod_name
                    dict_data["review_date"] = review_date
                    dict_data["user_name"] = user_name
                    dict_data["rating"] = rating
                    dict_data["headline"] = headline
                    dict_data["review_content"] = review_content
                    dict_data["answer"] = answer
                    dict_data["helpful_count"] = helpful_count
                    dict_data["seller_name"] = seller_name
                    dict_data["image_count"] = image_count

                    self.sd.save(datas=dict_data)
                    print(f"[SUCCESS] ë¦¬ë·° ì €ì¥ ì™„ë£Œ: {user_name} - {rating}ì ")

                page_delay = random.uniform(self.page_delay_min, self.page_delay_max)
                print(f"[DEBUG] ë‹¤ìŒ í˜ì´ì§€ê¹Œì§€ {page_delay:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(page_delay)
                return True

            except ProxyError as e:
                print(f"[ERROR] í”„ë¡ì‹œ ì˜¤ë¥˜: {e}")
                return "proxy_error"

            except Timeout as e:
                attempt += 1
                print(f"[ERROR] íƒ€ì„ì•„ì›ƒ ë°œìƒ (ì‹œë„ {attempt}/{self.retries}): {e}")
                if attempt < self.retries:
                    retry_delay = random.uniform(3.0, 6.0)
                    print(f"[DEBUG] íƒ€ì„ì•„ì›ƒ ë³µêµ¬ë¥¼ ìœ„í•´ {retry_delay:.1f}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(retry_delay)
                else:
                    print(f"[ERROR] íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•œ í˜ì´ì§€ {now_page} í¬ë¡¤ë§ ì‹¤íŒ¨.")
                    return False

            except ConnectionError as e:
                if self.use_proxy and ("proxy" in str(e).lower() or "407" in str(e)):
                    print(f"[ERROR] í”„ë¡ì‹œ ì—°ê²° ì˜¤ë¥˜: {e}")
                    return "proxy_error"
                else:
                    attempt += 1
                    print(f"[ERROR] ì—°ê²° ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt}/{self.retries}): {e}")
                    if attempt < self.retries:
                        retry_delay = random.uniform(4.0, 8.0)
                        print(f"[DEBUG] ì—°ê²° ë³µêµ¬ë¥¼ ìœ„í•´ {retry_delay:.1f}ì´ˆ ëŒ€ê¸°...")
                        time.sleep(retry_delay)
                    else:
                        print(f"[ERROR] ì—°ê²° ì˜¤ë¥˜ë¡œ ì¸í•œ í˜ì´ì§€ {now_page} í¬ë¡¤ë§ ì‹¤íŒ¨.")
                        return False

            except RequestException as e:
                attempt += 1
                print(f"[ERROR] ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt}/{self.retries}): {e}")
                if attempt < self.retries:
                    retry_delay = random.uniform(self.delay_min, self.delay_max)
                    print(f"[DEBUG] {retry_delay:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(retry_delay)
                else:
                    print(f"[ERROR] ìµœëŒ€ ìš”ì²­ íšŸìˆ˜ ì´ˆê³¼! í˜ì´ì§€ {now_page} í¬ë¡¤ë§ ì‹¤íŒ¨.")
                    return False
            except Exception as e:
                print(f"[ERROR] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return False

        return False

    @staticmethod
    def clear_console() -> None:
        command: str = "clear"
        if os.name in ("nt", "dos"):
            command = "cls"
        try:
            os.system(command=command)
        except:
            pass

    def input_review_url(self) -> str:
        while True:
            try:
                self.clear_console()
            except:
                pass

            review_url: str = input(
                "ì›í•˜ì‹œëŠ” ìƒí’ˆì˜ URL ì£¼ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”\n\n"
                "Ex)\n"
                "https://www.coupang.com/vp/products/7335597976?itemId=18741704367&vendorItemId=85873964906\n\n"
                "URL: "
            )
            if not review_url.strip():
                print("[ERROR] URL ì£¼ì†Œê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                time.sleep(2)
                continue

            if "coupang.com" not in review_url:
                print("[ERROR] ì˜¬ë°”ë¥¸ ì¿ íŒ¡ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”")
                time.sleep(2)
                continue

            return review_url.strip()


class SaveData:
    def __init__(self) -> None:
        self.wb: Workbook = Workbook()
        self.ws = self.wb.active
        self.ws.append([
            "ìƒí’ˆëª…", "êµ¬ë§¤ìƒí’ˆëª…", "ì‘ì„±ì¼ì", "êµ¬ë§¤ìëª…", "í‰ì ",
            "í—¤ë“œë¼ì¸", "ë¦¬ë·°ë‚´ìš©", "ë§›ë§Œì¡±ë„", "ë„ì›€ìˆ˜", "íŒë§¤ì", "ì´ë¯¸ì§€ìˆ˜"
        ])
        self.row: int = 2
        self.dir_name: str = "Coupang-reviews"
        self.create_directory()

    def create_directory(self) -> None:
        if not os.path.exists(self.dir_name):
            os.makedirs(self.dir_name)
            print(f"[INFO] ë””ë ‰í† ë¦¬ ìƒì„±: {self.dir_name}")

    def save(self, datas: dict[str, str | int]) -> None:
        try:
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', datas["title"])
            file_name: str = os.path.join(self.dir_name, safe_title + ".xlsx")

            self.ws[f"A{self.row}"] = datas["title"]
            self.ws[f"B{self.row}"] = datas["prod_name"]
            self.ws[f"C{self.row}"] = datas["review_date"]
            self.ws[f"D{self.row}"] = datas["user_name"]
            self.ws[f"E{self.row}"] = datas["rating"]
            self.ws[f"F{self.row}"] = datas["headline"]
            self.ws[f"G{self.row}"] = datas["review_content"]
            self.ws[f"H{self.row}"] = datas["answer"]
            self.ws[f"I{self.row}"] = datas["helpful_count"]
            self.ws[f"J{self.row}"] = datas["seller_name"]
            self.ws[f"K{self.row}"] = datas["image_count"]

            self.row += 1
            self.wb.save(filename=file_name)

        except Exception as e:
            print(f"[ERROR] ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def __del__(self) -> None:
        try:
            if hasattr(self, 'wb'):
                self.wb.close()
        except:
            pass


if __name__ == "__main__":
    try:
        print("=" * 60)
        print("ğŸ›’ ì¿ íŒ¡ ë¦¬ë·° í¬ë¡¤ëŸ¬ v3.4 (í”„ë¡ì‹œ ë¡œí…Œì´ì…˜ ì§€ì›)")
        print("=" * 60)

        coupang = Coupang()
        coupang.start()

        print("\n" + "=" * 60)
        print("âœ… í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ“ ê²°ê³¼ íŒŒì¼ì€ 'Coupang-reviews' í´ë”ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
        print("=" * 60)

    except KeyboardInterrupt:
        print("\n[INFO] ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n[ERROR] í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
    finally:
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")